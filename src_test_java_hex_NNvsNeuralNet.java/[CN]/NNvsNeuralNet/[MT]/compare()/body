{
  NN.Activation[] activations={NN.Activation.RectifierWithDropout,NN.Activation.Tanh,NN.Activation.Rectifier,NN.Activation.TanhWithDropout};
  NN.Loss[] losses={NN.Loss.MeanSquare,NN.Loss.CrossEntropy};
  NN.InitialWeightDistribution[] dists={NN.InitialWeightDistribution.Normal,NN.InitialWeightDistribution.Uniform,NN.InitialWeightDistribution.UniformAdaptive};
  double[] initial_weight_scales={1e-3 + 1e-2 * new Random().nextFloat()};
  double[] holdout_ratios={0.7 + 0.1 * new Random().nextFloat()};
  int[][] hiddens={{1},{1 + new Random().nextInt(50)},{17,13},{20,10,5}};
  double[] rates={0.005 + 1e-2 * new Random().nextFloat()};
  int[] epochs={10 + new Random().nextInt(5)};
  boolean threaded=false;
  int num_repeats=1;
  String[] files={"smalldata/iris/iris.csv"};
  double p0=0;
  long pR=0;
  double p1=0;
  double l1=1e-5 * new Random().nextFloat();
  double l2=1e-5 * new Random().nextFloat();
  double max_w2=new Random().nextInt(50);
  double input_dropout=new Random().nextFloat() * 0.5;
  double rate_annealing=1e-7 + new Random().nextFloat() * 1e-6;
  for (  NN.Activation activation : activations) {
    for (    NN.Loss loss : losses) {
      for (      NN.InitialWeightDistribution dist : dists) {
        for (        double scale : initial_weight_scales) {
          for (          double holdout_ratio : holdout_ratios) {
            for (            int[] hidden : hiddens) {
              for (              int epoch : epochs) {
                for (                double rate : rates) {
                  for (                  String file : files) {
                    double referror=0, myerror=0;
                    double[] a=new double[hidden.length + 2];
                    double[] b=new double[hidden.length + 2];
                    double[] ba=new double[hidden.length + 2];
                    double[] bb=new double[hidden.length + 2];
                    long numweights=0, numbiases=0;
                    for (int repeat=0; repeat < num_repeats; ++repeat) {
                      long seed=new Random().nextLong();
                      Log.info("");
                      Log.info("STARTING.");
                      Log.info("Running with " + activation.name() + " activation function and "+ loss.name()+ " loss function.");
                      Log.info("Initialization with " + dist.name() + " distribution and "+ scale+ " scale, holdout ratio "+ holdout_ratio);
                      Log.info("Using seed " + seed);
                      Key kfile=NFSFileVec.make(find_test_file(file));
                      Frame frame=ParseDataset2.parse(Key.make(),new Key[]{kfile});
                      _train=sampleFrame(frame,(long)(frame.numRows() * holdout_ratio),seed);
                      _test=sampleFrame(frame,(long)(frame.numRows() * (1 - holdout_ratio)),seed + 1);
                      Neurons[] neurons;
                      NNModel mymodel;
{
                        NN p=new NN();
                        p.source=(Frame)_train.clone();
                        p.response=_train.lastVec();
                        p.ignored_cols=null;
                        Frame fr=FrameTask.DataInfo.prepareFrame(p.source,p.response,p.ignored_cols,true,p.ignore_const_cols);
                        p._dinfo=new FrameTask.DataInfo(fr,1,true);
                        p.seed=seed;
                        p.hidden=hidden;
                        p.rate=rate;
                        p.activation=activation;
                        p.max_w2=max_w2;
                        p.epochs=epoch;
                        p.input_dropout_ratio=input_dropout;
                        p.rate_annealing=rate_annealing;
                        p.loss=loss;
                        p.l1=l1;
                        p.l2=l2;
                        p.momentum_start=p0;
                        p.momentum_ramp=pR;
                        p.momentum_stable=p1;
                        p.initial_weight_distribution=dist;
                        p.initial_weight_scale=scale;
                        p.classification=true;
                        p.diagnostics=true;
                        p.validation=null;
                        p.fast_mode=true;
                        p.sync_samples=0;
                        p.ignore_const_cols=false;
                        p.shuffle_training_data=false;
                        p.exec();
                        mymodel=UKV.get(p.dest());
                        neurons=NNTask.makeNeuronsForTesting(mymodel.model_info());
                      }
                      Layer[] ls;
                      NeuralNetModel refmodel;
                      NeuralNet p=new NeuralNet();
{
                        Vec[] data=Utils.remove(_train.vecs(),_train.vecs().length - 1);
                        Vec labels=_train.lastVec();
                        p.seed=seed;
                        p.hidden=hidden;
                        p.rate=rate;
                        p.max_w2=max_w2;
                        p.epochs=epoch;
                        p.input_dropout_ratio=input_dropout;
                        p.rate_annealing=rate_annealing;
                        p.l1=l1;
                        p.l2=l2;
                        p.momentum_start=p0;
                        p.momentum_ramp=pR;
                        p.momentum_stable=p1;
                        if (dist == NN.InitialWeightDistribution.Normal)                         p.initial_weight_distribution=InitialWeightDistribution.Normal;
 else                         if (dist == NN.InitialWeightDistribution.Uniform)                         p.initial_weight_distribution=InitialWeightDistribution.Uniform;
 else                         if (dist == NN.InitialWeightDistribution.UniformAdaptive)                         p.initial_weight_distribution=InitialWeightDistribution.UniformAdaptive;
                        p.initial_weight_scale=scale;
                        p.diagnostics=true;
                        p.classification=true;
                        if (loss == NN.Loss.MeanSquare)                         p.loss=Loss.MeanSquare;
 else                         if (loss == NN.Loss.CrossEntropy)                         p.loss=Loss.CrossEntropy;
                        ls=new Layer[hidden.length + 2];
                        ls[0]=new Layer.VecsInput(data,null);
                        for (int i=0; i < hidden.length; ++i) {
                          if (activation == NN.Activation.Tanh) {
                            p.activation=NeuralNet.Activation.Tanh;
                            ls[1 + i]=new Layer.Tanh(hidden[i]);
                          }
 else                           if (activation == NN.Activation.TanhWithDropout) {
                            p.activation=Activation.TanhWithDropout;
                            ls[1 + i]=new Layer.TanhDropout(hidden[i]);
                          }
 else                           if (activation == NN.Activation.Rectifier) {
                            p.activation=Activation.Rectifier;
                            ls[1 + i]=new Layer.Rectifier(hidden[i]);
                          }
 else                           if (activation == NN.Activation.RectifierWithDropout) {
                            p.activation=Activation.RectifierWithDropout;
                            ls[1 + i]=new Layer.RectifierDropout(hidden[i]);
                          }
                        }
                        ls[ls.length - 1]=new Layer.VecSoftmax(labels,null);
                        for (int i=0; i < ls.length; i++) {
                          ls[i].init(ls,i,p);
                        }
                        Trainer trainer;
                        if (threaded)                         trainer=new Trainer.Threaded(ls,p.epochs,null,-1);
 else                         trainer=new Trainer.Direct(ls,p.epochs,null);
                        trainer.start();
                        trainer.join();
                        refmodel=new NeuralNetModel(null,null,_train,ls,p);
                      }
                      for (int n=1; n < ls.length; ++n) {
                        Neurons l=neurons[n];
                        Layer ref=ls[n];
                        for (int o=0; o < l._a.length; o++) {
                          for (int i=0; i < l._previous._a.length; i++) {
                            a[n]+=ref._w[o * l._previous._a.length + i];
                            b[n]+=l._w[o * l._previous._a.length + i];
                            numweights++;
                          }
                          ba[n]+=ref._b[o];
                          bb[n]+=l._b[o];
                          numbiases++;
                        }
                      }
{
                        Frame fpreds=mymodel.score(_test);
                        water.api.ConfusionMatrix CM=new water.api.ConfusionMatrix();
                        CM.actual=_test;
                        CM.vactual=_test.lastVec();
                        CM.predict=fpreds;
                        CM.vpredict=fpreds.vecs()[0];
                        CM.serve();
                        StringBuilder sb=new StringBuilder();
                        myerror+=CM.toASCII(sb);
                        for (                        String s : sb.toString().split("\n"))                         Log.info(s);
                        fpreds.delete();
                      }
{
                        Log.info("\nNeuralNet Scoring:");
                        final Frame[] adapted=refmodel.adapt(_test,false);
                        Vec[] data=Utils.remove(_test.vecs(),_test.vecs().length - 1);
                        Vec labels=_test.vecs()[_test.vecs().length - 1];
                        Layer.VecsInput input=(Layer.VecsInput)ls[0];
                        input.vecs=data;
                        input._len=data[0].length();
                        ((Layer.VecSoftmax)ls[ls.length - 1]).vec=labels;
                        long[][] cm;
                        int classes=ls[ls.length - 1].units;
                        cm=new long[classes][classes];
                        NeuralNet.Errors test=NeuralNet.eval(ls,0,cm);
                        Log.info("\nNeuralNet Confusion Matrix:");
                        Log.info(new ConfusionMatrix(cm).toString());
                        referror+=test.classification;
                        adapted[1].delete();
                      }
                      mymodel.delete();
                      refmodel.delete();
                      _train.delete();
                      _test.delete();
                      frame.delete();
                    }
                    myerror/=(double)num_repeats;
                    referror/=(double)num_repeats;
                    final double abseps=threaded ? 1e-3 : 1e-13;
                    final double releps=threaded ? 1e-3 : 1e-13;
                    Log.info("NeuralNet test error " + referror);
                    Log.info("NN        test error " + myerror);
                    compareVal(referror,myerror,abseps,releps);
                    for (int n=1; n < hidden.length + 2; ++n) {
                      Log.info("NeuralNet mean weight for layer " + n + ": "+ a[n] / numweights);
                      Log.info("NN        mean weight for layer " + n + ": "+ b[n] / numweights);
                      Log.info("NeuralNet mean bias for layer " + n + ": "+ ba[n] / numbiases);
                      Log.info("NN        mean bias for layer " + n + ": "+ bb[n] / numbiases);
                      compareVal(a[n] / numweights,b[n] / numweights,abseps,releps);
                      compareVal(ba[n] / numbiases,bb[n] / numbiases,abseps,releps);
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}
