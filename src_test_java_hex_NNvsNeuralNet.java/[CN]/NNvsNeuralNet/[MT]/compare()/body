{
  NN.Activation[] activations={NN.Activation.Maxout,NN.Activation.MaxoutWithDropout,NN.Activation.RectifierWithDropout,NN.Activation.Tanh,NN.Activation.Rectifier,NN.Activation.TanhWithDropout};
  NN.Loss[] losses={NN.Loss.MeanSquare,NN.Loss.CrossEntropy};
  NN.InitialWeightDistribution[] dists={NN.InitialWeightDistribution.Normal,NN.InitialWeightDistribution.Uniform,NN.InitialWeightDistribution.UniformAdaptive};
  double[] initial_weight_scales={1e-3 + 1e-2 * new Random().nextFloat()};
  double[] holdout_ratios={0.7 + 0.2 * new Random().nextFloat()};
  int[][] hiddens={{1},{1 + new Random().nextInt(50)},{17,13},{20,10,5}};
  double[] rates={0.005 + 1e-2 * new Random().nextFloat()};
  int[] epochs={5 + new Random().nextInt(5)};
  double[] input_dropouts={0,new Random().nextFloat() * 0.5};
  double p0=0.5 * new Random().nextFloat();
  long pR=1000 + new Random().nextInt(1000);
  double p1=0.5 + 0.49 * new Random().nextFloat();
  double l1=1e-5 * new Random().nextFloat();
  double l2=1e-5 * new Random().nextFloat();
  double max_w2=new Random().nextInt(50);
  double rate_annealing=1e-7 + new Random().nextFloat() * 1e-6;
  boolean threaded=false;
  int num_repeats=1;
  String[] files={"smalldata/iris/iris.csv","smalldata/neural/two_spiral.data"};
  for (  NN.Activation activation : activations) {
    for (    NN.Loss loss : losses) {
      for (      NN.InitialWeightDistribution dist : dists) {
        for (        double scale : initial_weight_scales) {
          for (          double holdout_ratio : holdout_ratios) {
            for (            double input_dropout : input_dropouts) {
              for (              int[] hidden : hiddens) {
                for (                int epoch : epochs) {
                  for (                  double rate : rates) {
                    for (                    String file : files) {
                      double reftrainerr=0, trainerr=0;
                      double reftesterr=0, testerr=0;
                      double[] a=new double[hidden.length + 2];
                      double[] b=new double[hidden.length + 2];
                      double[] ba=new double[hidden.length + 2];
                      double[] bb=new double[hidden.length + 2];
                      long numweights=0, numbiases=0;
                      for (int repeat=0; repeat < num_repeats; ++repeat) {
                        long seed=new Random().nextLong();
                        Log.info("");
                        Log.info("STARTING.");
                        Log.info("Running with " + activation.name() + " activation function and "+ loss.name()+ " loss function.");
                        Log.info("Initialization with " + dist.name() + " distribution and "+ scale+ " scale, holdout ratio "+ holdout_ratio);
                        Log.info("Using seed " + seed);
                        Key kfile=NFSFileVec.make(find_test_file(file));
                        Frame frame=ParseDataset2.parse(Key.make(),new Key[]{kfile});
                        _train=sampleFrame(frame,(long)(frame.numRows() * holdout_ratio),seed);
                        _test=sampleFrame(frame,(long)(frame.numRows() * (1 - holdout_ratio)),seed + 1);
                        Neurons[] neurons;
                        NNModel mymodel;
{
                          NN p=new NN();
                          p.source=(Frame)_train.clone();
                          p.response=_train.lastVec();
                          p.ignored_cols=null;
                          Frame fr=FrameTask.DataInfo.prepareFrame(p.source,p.response,p.ignored_cols,true,p.ignore_const_cols);
                          p._dinfo=new FrameTask.DataInfo(fr,1,true);
                          p.seed=seed;
                          p.hidden=hidden;
                          p.rate=rate;
                          p.activation=activation;
                          p.max_w2=max_w2;
                          p.epochs=epoch;
                          p.input_dropout_ratio=input_dropout;
                          p.rate_annealing=rate_annealing;
                          p.loss=loss;
                          p.l1=l1;
                          p.l2=l2;
                          p.momentum_start=p0;
                          p.momentum_ramp=pR;
                          p.momentum_stable=p1;
                          p.initial_weight_distribution=dist;
                          p.initial_weight_scale=scale;
                          p.classification=true;
                          p.diagnostics=true;
                          p.validation=null;
                          p.fast_mode=true;
                          p.sync_samples=0;
                          p.ignore_const_cols=false;
                          p.shuffle_training_data=false;
                          p.exec();
                          mymodel=UKV.get(p.dest());
                          neurons=NNTask.makeNeuronsForTesting(mymodel.model_info());
                        }
                        Layer[] ls;
                        NeuralNetModel refmodel;
                        NeuralNet p=new NeuralNet();
{
                          Vec[] data=Utils.remove(_train.vecs(),_train.vecs().length - 1);
                          Vec labels=_train.lastVec();
                          p.seed=seed;
                          p.hidden=hidden;
                          p.rate=rate;
                          p.max_w2=max_w2;
                          p.epochs=epoch;
                          p.input_dropout_ratio=input_dropout;
                          p.rate_annealing=rate_annealing;
                          p.l1=l1;
                          p.l2=l2;
                          p.momentum_start=p0;
                          p.momentum_ramp=pR;
                          p.momentum_stable=p1;
                          if (dist == NN.InitialWeightDistribution.Normal)                           p.initial_weight_distribution=InitialWeightDistribution.Normal;
 else                           if (dist == NN.InitialWeightDistribution.Uniform)                           p.initial_weight_distribution=InitialWeightDistribution.Uniform;
 else                           if (dist == NN.InitialWeightDistribution.UniformAdaptive)                           p.initial_weight_distribution=InitialWeightDistribution.UniformAdaptive;
                          p.initial_weight_scale=scale;
                          p.diagnostics=true;
                          p.classification=true;
                          if (loss == NN.Loss.MeanSquare)                           p.loss=Loss.MeanSquare;
 else                           if (loss == NN.Loss.CrossEntropy)                           p.loss=Loss.CrossEntropy;
                          ls=new Layer[hidden.length + 2];
                          ls[0]=new Layer.VecsInput(data,null);
                          for (int i=0; i < hidden.length; ++i) {
                            if (activation == NN.Activation.Tanh) {
                              p.activation=NeuralNet.Activation.Tanh;
                              ls[1 + i]=new Layer.Tanh(hidden[i]);
                            }
 else                             if (activation == NN.Activation.TanhWithDropout) {
                              p.activation=Activation.TanhWithDropout;
                              ls[1 + i]=new Layer.TanhDropout(hidden[i]);
                            }
 else                             if (activation == NN.Activation.Rectifier) {
                              p.activation=Activation.Rectifier;
                              ls[1 + i]=new Layer.Rectifier(hidden[i]);
                            }
 else                             if (activation == NN.Activation.RectifierWithDropout) {
                              p.activation=Activation.RectifierWithDropout;
                              ls[1 + i]=new Layer.RectifierDropout(hidden[i]);
                            }
 else                             if (activation == NN.Activation.Maxout) {
                              p.activation=Activation.Maxout;
                              ls[1 + i]=new Layer.Maxout(hidden[i]);
                            }
 else                             if (activation == NN.Activation.MaxoutWithDropout) {
                              p.activation=Activation.MaxoutWithDropout;
                              ls[1 + i]=new Layer.MaxoutDropout(hidden[i]);
                            }
                          }
                          ls[ls.length - 1]=new Layer.VecSoftmax(labels,null);
                          for (int i=0; i < ls.length; i++) {
                            ls[i].init(ls,i,p);
                          }
                          Trainer trainer;
                          if (threaded)                           trainer=new Trainer.Threaded(ls,p.epochs,null,-1);
 else                           trainer=new Trainer.Direct(ls,p.epochs,null);
                          trainer.start();
                          trainer.join();
                          refmodel=new NeuralNetModel(null,null,_train,ls,p);
                        }
                        for (int n=1; n < ls.length; ++n) {
                          Neurons l=neurons[n];
                          Layer ref=ls[n];
                          for (int o=0; o < l._a.length; o++) {
                            for (int i=0; i < l._previous._a.length; i++) {
                              a[n]+=ref._w[o * l._previous._a.length + i];
                              b[n]+=l._w[o * l._previous._a.length + i];
                              numweights++;
                            }
                            ba[n]+=ref._b[o];
                            bb[n]+=l._b[o];
                            numbiases++;
                          }
                        }
{
                          Frame fpreds=mymodel.score(_train);
                          water.api.ConfusionMatrix CM=new water.api.ConfusionMatrix();
                          CM.actual=_train;
                          CM.vactual=_train.lastVec();
                          CM.predict=fpreds;
                          CM.vpredict=fpreds.vecs()[0];
                          CM.serve();
                          StringBuilder sb=new StringBuilder();
                          trainerr+=CM.toASCII(sb);
                          for (                          String s : sb.toString().split("\n"))                           Log.info(s);
                          fpreds.delete();
                          Frame fpreds2=mymodel.score(_test);
                          CM=new water.api.ConfusionMatrix();
                          CM.actual=_test;
                          CM.vactual=_test.lastVec();
                          CM.predict=fpreds2;
                          CM.vpredict=fpreds2.vecs()[0];
                          CM.serve();
                          sb=new StringBuilder();
                          testerr+=CM.toASCII(sb);
                          for (                          String s : sb.toString().split("\n"))                           Log.info(s);
                          fpreds2.delete();
                        }
{
                          Log.info("\nNeuralNet Scoring:");
                          NeuralNet.Errors train=NeuralNet.eval(ls,0,null);
                          reftrainerr+=train.classification;
                          final Frame[] adapted=refmodel.adapt(_test,false);
                          Vec[] data=Utils.remove(_test.vecs(),_test.vecs().length - 1);
                          Vec labels=_test.vecs()[_test.vecs().length - 1];
                          Layer.VecsInput input=(Layer.VecsInput)ls[0];
                          input.vecs=data;
                          input._len=data[0].length();
                          ((Layer.VecSoftmax)ls[ls.length - 1]).vec=labels;
                          long[][] cm;
                          int classes=ls[ls.length - 1].units;
                          cm=new long[classes][classes];
                          NeuralNet.Errors test=NeuralNet.eval(ls,0,cm);
                          Log.info("\nNeuralNet Confusion Matrix:");
                          Log.info(new ConfusionMatrix(cm).toString());
                          reftesterr+=test.classification;
                          adapted[1].delete();
                        }
                        mymodel.delete();
                        refmodel.delete();
                        _train.delete();
                        _test.delete();
                        frame.delete();
                      }
                      trainerr/=(double)num_repeats;
                      reftrainerr/=(double)num_repeats;
                      testerr/=(double)num_repeats;
                      reftesterr/=(double)num_repeats;
                      final double abseps=threaded ? 1e-2 : 1e-13;
                      final double releps=threaded ? 1e-2 : 1e-13;
                      Log.info("NeuralNet train error " + reftrainerr);
                      Log.info("NN        train error " + trainerr);
                      compareVal(reftrainerr,trainerr,abseps,releps);
                      Log.info("NeuralNet test error " + reftesterr);
                      Log.info("NN        test error " + testerr);
                      compareVal(reftrainerr,trainerr,abseps,releps);
                      for (int n=1; n < hidden.length + 2; ++n) {
                        Log.info("NeuralNet mean weight for layer " + n + ": "+ a[n] / numweights);
                        Log.info("NN        mean weight for layer " + n + ": "+ b[n] / numweights);
                        Log.info("NeuralNet mean bias for layer " + n + ": "+ ba[n] / numbiases);
                        Log.info("NN        mean bias for layer " + n + ": "+ bb[n] / numbiases);
                        compareVal(a[n] / numweights,b[n] / numweights,abseps,releps);
                        compareVal(ba[n] / numbiases,bb[n] / numbiases,abseps,releps);
                      }
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}
