{
  Logger.getRootLogger().setLevel(Level.ALL);
  System.setProperty("HADOOP_USER_NAME",config.user);
  Configuration conf=new Configuration();
  conf.set("fs.default.name",config.name_server);
  conf.set("mapred.job.tracker",config.tracker);
  conf.set("mapreduce.framework.name","classic");
  conf.set("hadoop.job.ugi",config.user + "," + config.password);
  conf.setInt("mapred.tasktracker.map.tasks.maximum",1);
  conf.set("mapred.jar","/home/cypof/h2o/target/h2o.jar");
  conf.set("mapred.child.java.opts","-Xms" + config.memory + "m -Xmx"+ config.memory+ "m");
  conf.set("mapred.job.map.memory.mb","" + config.memory);
  conf.set("mapred.job.reduce.memory.mb","" + config.memory);
  conf.set("mapred.fairscheduler.locality.delay","120000");
  String hosts="";
  URI tracker=new URI(config.tracker);
  JobClient client=new JobClient(new InetSocketAddress(tracker.getHost(),tracker.getPort()),conf);
  Collection<String> names=client.getClusterStatus(true).getActiveTrackerNames();
  for (  String name : names)   hosts+=name.substring("tracker_".length(),name.indexOf(':')) + ',';
  System.out.println("Deploying to " + hosts);
  conf.set(HOSTS_KEY,hosts);
  conf.set(PORT_KEY,"" + config.port);
  ToolRunner.run(conf,new HadoopTool(),args);
  String url="http://" + hosts.substring(0,hosts.indexOf(',')) + ":"+ config.port+ "/";
  for (; ; ) {
    if (size(url) == names.size())     break;
    Thread.sleep(300);
  }
  System.out.println("H2O running, can be reached at " + url);
  if (config.script != null) {
    System.out.println("Sending " + config.script);
    String res=post(url,config.script,0);
    JSONObject json=new JSONObject(res);
    JSONObject resp=json.getJSONObject(Constants.RESPONSE);
    String status=resp.getString(Constants.STATUS);
    System.out.println("Status: " + status);
    if (status == Status.error.name()) {
      System.out.println("Response: " + res);
    }
  }
}
