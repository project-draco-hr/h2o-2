def test_flashgordon(self):
    avgMichalSize = 116561140
    avgSynSize = 4020000
    csvFilenameList = [('100.dat.gz', 'dat_1', (1 * avgSynSize), 700), ('11[0-9].dat.gz', 'dat_10', (10 * avgSynSize), 700), ('1[32][0-9].dat.gz', 'dat_20', (20 * avgSynSize), 800), ('1[5-9][0-9].dat.gz', 'dat_50', (50 * avgSynSize), 900)]
    print 'Using the -.gz files from s3'
    USE_S3 = False
    noPoll = True
    benchmarkLogging = ['cpu', 'disk']
    bucket = 'flashgordon'
    if USE_S3:
        URI = (('s3://' + bucket) + '/')
        protocol = 's3'
    else:
        URI = (('s3n://' + bucket) + '/')
        protocol = 's3n/hdfs'
    trialMax = 1
    for (i, (csvFilepattern, csvFilename, totalBytes, timeoutSecs)) in enumerate(csvFilenameList):
        for tryHeap in [28]:
            print '\n', tryHeap, 'GB heap, 1 jvm per host, import', protocol, 'then parse'
            h2o_hosts.build_cloud_with_hosts(node_count=1, java_heap_GB=tryHeap, enable_benchmark_log=True, timeoutSecs=120, retryDelaySecs=10, hdfs_name_node='10.78.14.235:9000', hdfs_version='0.20.2')
            h2o.nodes[0].sandbox_ignore_errors = True
            for trial in range(trialMax):
                if USE_S3:
                    importResult = h2o.nodes[0].import_s3(bucket)
                else:
                    importResult = h2o.nodes[0].import_hdfs(URI)
                s3nFullList = importResult['files']
                for k in s3nFullList:
                    key = k['key']
                    if (csvFilepattern in key):
                        print "example file we'll use:", key
                        break
                    else:
                        pass
                self.assertGreater(len(s3nFullList), 8, "Didn't see more than 8 files in s3n?")
                s3nKey = (URI + csvFilepattern)
                key2 = (((csvFilename + '_') + str(trial)) + '.hex')
                print 'Loading', protocol, 'key:', s3nKey, 'to', key2
                start = time.time()
                parseKey = h2o.nodes[0].parse(s3nKey, key2, timeoutSecs=timeoutSecs, retryDelaySecs=10, pollTimeoutSecs=60, noPoll=noPoll, benchmarkLogging=benchmarkLogging)
                if noPoll:
                    time.sleep(1)
                    h2o.check_sandbox_for_errors()
                    (csvFilepattern, csvFilename, totalBytes2, timeoutSecs) = csvFilenameList[(i + 1)]
                    s3nKey = (URI + csvFilepattern)
                    key2 = (((csvFilename + '_') + str(trial)) + '.hex')
                    print 'Loading', protocol, 'key:', s3nKey, 'to', key2
                    parse2Key = h2o.nodes[0].parse(s3nKey, key2, timeoutSecs=timeoutSecs, retryDelaySecs=10, pollTimeoutSecs=60, noPoll=noPoll, benchmarkLogging=benchmarkLogging)
                    time.sleep(1)
                    h2o.check_sandbox_for_errors()
                    (csvFilepattern, csvFilename, totalBytes3, timeoutSecs) = csvFilenameList[(i + 2)]
                    s3nKey = (URI + csvFilepattern)
                    key2 = (((csvFilename + '_') + str(trial)) + '.hex')
                    print 'Loading', protocol, 'key:', s3nKey, 'to', key2
                    parse3Key = h2o.nodes[0].parse(s3nKey, key2, timeoutSecs=timeoutSecs, retryDelaySecs=10, pollTimeoutSecs=60, noPoll=noPoll, benchmarkLogging=benchmarkLogging)
                elapsed = (time.time() - start)
                print s3nKey, 'parse time:', parseKey['response']['time']
                print 'parse result:', parseKey['destination_key']
                print 'Parse #', trial, 'completed in', ('%6.2f' % elapsed), 'seconds.', ('%d pct. of timeout' % ((elapsed * 100) / timeoutSecs))
                if noPoll:
                    time.sleep(2)
                    h2o_jobs.pollWaitJobs(pattern=csvFilename, timeoutSecs=timeoutSecs, benchmarkLogging=benchmarkLogging)
                    totalBytes += (totalBytes2 + totalBytes3)
                    elapsed = (time.time() - start)
                    h2o.check_sandbox_for_errors()
                if (totalBytes is not None):
                    fileMBS = ((totalBytes / 1000000.0) / elapsed)
                    print '\nMB/sec (before uncompress)', ('%6.2f' % fileMBS)
                    h2o.cloudPerfH2O.message('{:d} jvms, {:d}GB heap, {:s} {:s} {:6.2f} MB/sec for {:6.2f} secs'.format(len(h2o.nodes), tryHeap, csvFilepattern, csvFilename, fileMBS, elapsed))
                if (not noPoll):
                    inspect = h2o_cmd.runInspect(key=parseKey['destination_key'])
                print 'Deleting key in H2O so we get it from S3 (if ec2) or nfs again.', 'Otherwise it would just parse the cached key.'
                storeView = h2o.nodes[0].store_view()
                for k in s3nFullList:
                    deleteKey = k['key']
                    if ((csvFilename in deleteKey) and (not ('.hex' in key))):
                        pass
            h2o.tear_down_cloud()
            time.sleep(120)
