def test_flashgordon(self):
    avgMichalSize = 116561140
    avgSynSize = 4020000
    csvFilenameList = [('100.dat.gz', 'dat_1', (1 * avgSynSize), 700), ('11[0-9].dat.gz', 'dat_10', (10 * avgSynSize), 700), ('1[32][0-9].dat.gz', 'dat_20', (20 * avgSynSize), 800), ('1[5-9][0-9].dat.gz', 'dat_50', (50 * avgSynSize), 900)]
    print 'Using the -.gz files from s3'
    USE_S3 = False
    noPoll = True
    benchmarkLogging = ['cpu', 'disk']
    bucket = 'flashgordon'
    trialMax = 1
    for (i, (csvFilepattern, csvFilename, totalBytes, timeoutSecs)) in enumerate(csvFilenameList):
        for tryHeap in [28]:
            print '\n', tryHeap, 'GB heap, 1 jvm per host, import', protocol, 'then parse'
            h2o_hosts.build_cloud_with_hosts(node_count=1, java_heap_GB=tryHeap, enable_benchmark_log=True, timeoutSecs=120, retryDelaySecs=10)
            h2o.nodes[0].sandboxIgnoreErrors = True
            for trial in range(trialMax):
                if USE_S3:
                    schema = 's3'
                else:
                    schema = 's3n'
                hex_key = (((csvFilename + '_') + str(trial)) + '.hex')
                start = time.time()
                parseResult = h2i.import_parse(bucket=bucket, path=csvFilepattern, schema=schema, hex_key=hex_key, timeoutSecs=timeoutSecs, retryDelaySecs=10, pollTimeoutSecs=60, noPoll=noPoll, benchmarkLogging=benchmarkLogging)
                if noPoll:
                    time.sleep(1)
                    h2o.check_sandbox_for_errors()
                    (csvFilepattern, csvFilename, totalBytes2, timeoutSecs) = csvFilenameList[(i + 1)]
                    hex_key = (((csvFilename + '_') + str(trial)) + '.hex')
                    parseResult = h2i.import_parse(bucket=bucket, path=csvFilepattern, schema=schema, hex_key=hex_key, timeoutSecs=timeoutSecs, retryDelaySecs=10, pollTimeoutSecs=60, noPoll=noPoll, benchmarkLogging=benchmarkLogging)
                    time.sleep(1)
                    h2o.check_sandbox_for_errors()
                    (csvFilepattern, csvFilename, totalBytes3, timeoutSecs) = csvFilenameList[(i + 2)]
                    s3nKey = (URI + csvFilepattern)
                    hex_key = (((csvFilename + '_') + str(trial)) + '.hex')
                    parseResult = h2i.import_parse(bucket=bucket, path=csvFilepattern, schema=schema, hex_key=hex_key, timeoutSecs=timeoutSecs, retryDelaySecs=10, pollTimeoutSecs=60, noPoll=noPoll, benchmarkLogging=benchmarkLogging)
                elapsed = (time.time() - start)
                print 'parse result:', parseResult['destination_key']
                print 'Parse #', trial, 'completed in', ('%6.2f' % elapsed), 'seconds.', ('%d pct. of timeout' % ((elapsed * 100) / timeoutSecs))
                if noPoll:
                    time.sleep(2)
                    h2o_jobs.pollWaitJobs(pattern=csvFilename, timeoutSecs=timeoutSecs, benchmarkLogging=benchmarkLogging)
                    totalBytes += (totalBytes2 + totalBytes3)
                    elapsed = (time.time() - start)
                    h2o.check_sandbox_for_errors()
                if (totalBytes is not None):
                    fileMBS = ((totalBytes / 1000000.0) / elapsed)
                    print '\nMB/sec (before uncompress)', ('%6.2f' % fileMBS)
                    h2o.cloudPerfH2O.message('{:d} jvms, {:d}GB heap, {:s} {:s} {:6.2f} MB/sec for {:6.2f} secs'.format(len(h2o.nodes), tryHeap, csvFilepattern, csvFilename, fileMBS, elapsed))
                if (not noPoll):
                    inspect = h2o_cmd.runInspect(key=parseResult['destination_key'])
            h2o.tear_down_cloud()
            time.sleep(120)
