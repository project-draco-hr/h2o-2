def test_summary2_uniform(self):
    SYNDATASETS_DIR = h2o.make_syn_dir()
    tryList = [('runif.csv', 'A', 0, 100), ('runifA.csv', 'B', 0, 100), ('runifB.csv', 'C', 0, 100), ('runifC.csv', 'D', 0, 100)]
    timeoutSecs = 10
    trial = 1
    n = h2o.nodes[0]
    lenNodes = len(h2o.nodes)
    x = 0
    timeoutSecs = 60
    for (csvFilename, hex_key, expectedMin, expectedMax) in tryList:
        h2o.beta_features = False
        csvPathname = csvFilename
        parseResult = h2i.import_parse(bucket='smalldata', path=csvPathname, schema='put', hex_key=hex_key, timeoutSecs=10, doSummary=False)
        print csvFilename, 'parse time:', parseResult['response']['time']
        print "Parse result['destination_key']:", parseResult['destination_key']
        inspect = h2o_cmd.runInspect(None, parseResult['destination_key'])
        print ('\n' + csvFilename)
        numRows = inspect['num_rows']
        numCols = inspect['num_cols']
        h2o.beta_features = True
        summaryResult = h2o_cmd.runSummary(key=hex_key)
        h2o.verboseprint('summaryResult:', h2o.dump_json(summaryResult))
        summaries = summaryResult['summaries']
        for column in summaries:
            colname = column['colname']
            coltype = column['type']
            nacnt = column['nacnt']
            stats = column['stats']
            stattype = stats['type']
            mean = stats['mean']
            sd = stats['sd']
            zeros = stats['zeros']
            mins = stats['mins']
            maxs = stats['maxs']
            pct = stats['pct']
            pctile = stats['pctile']
            hstart = column['hstart']
            hstep = column['hstep']
            hbrk = column['hbrk']
            hcnt = column['hcnt']
            print csvFilename, 'colname:', colname, 'pctile:', pctile
            print 'pct:', pct
            print ''
            for b in hcnt:
                e = (0.1 * numRows)
            if (1 == 0):
                print 'pctile:', pctile
                print 'maxs:', maxs
                self.assertAlmostEqual(maxs[0], expectedMax, delta=0.2)
                print 'mins:', mins
                self.assertAlmostEqual(mins[0], expectedMin, delta=0.2)
                for v in pctile:
                    self.assertTrue((v >= expectedMin), ('Percentile value %s should all be >= the min dataset value %s' % (v, expectedMin)))
                    self.assertTrue((v <= expectedMax), ('Percentile value %s should all be <= the max dataset value %s' % (v, expectedMax)))
                eV1 = [1.0, 1.0, 1.0, 3.0, 4.0, 5.0, 7.0, 8.0, 9.0, 10.0, 10.0]
                if (expectedMin == 1):
                    eV = eV1
                elif (expectedMin == 0):
                    eV = [(e - 1) for e in eV1]
                elif (expectedMin == 2):
                    eV = [(e + 1) for e in eV1]
                else:
                    raise Exception(("Test doesn't have the expected percentileValues for expectedMin: %s" % expectedMin))
        trial += 1
        if DO_SCIPY_COMPARE:
            csvPathname1 = h2i.find_folder_and_filename('smalldata', csvPathname, returnFullPath=True)
            generate_scipy_comparison(csvPathname1)
