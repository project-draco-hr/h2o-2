def test_benchmark_import(self):
    avgMichalSizeUncompressed = 237270000
    avgMichalSize = 116561140
    avgSynSize = 4020000
    covtype200xSize = 15033863400
    synSize = 183
    if (1 == 1):
        bucket = 'home-0xdiag-datasets'
        importFolderPath = 'manyfiles-nflx-gz'
        print "Using .gz'ed files in", importFolderPath
        csvFilenameAll = [('*file_[12][01][0-7].dat.gz', 'file_32.dat.gz', (100 * avgMichalSize), 1800)]
    if (1 == 0):
        bucket = 'home-0xdiag-datasets'
        importFolderPath = 'more1_1200_link'
        print "Using .gz'ed files in", importFolderPath
        csvFilenameAll = [('*[3-4][0-4][0-9].dat.gz', 'file_100_A.dat.gz', (100 * avgMichalSize), 3600), ('*[3-4][0-4][0-9].dat.gz', 'file_100_B.dat.gz', (100 * avgMichalSize), 3600), ('*[3-4][0-5][0-9].dat.gz', 'file_120_A.dat.gz', (120 * avgMichalSize), 3600), ('*[3-4][0-5][0-9].dat.gz', 'file_120_B.dat.gz', (120 * avgMichalSize), 3600), ('*[3-4][0-6][0-9].dat.gz', 'file_140_A.dat.gz', (140 * avgMichalSize), 3600), ('*[3-4][0-6][0-9].dat.gz', 'file_140_B.dat.gz', (140 * avgMichalSize), 3600), ('*[3-4][0-7][0-9].dat.gz', 'file_160_A.dat.gz', (160 * avgMichalSize), 3600), ('*[3-4][0-7][0-9].dat.gz', 'file_160_B.dat.gz', (160 * avgMichalSize), 3600), ('*[3-4][0-8][0-9].dat.gz', 'file_180_A.dat.gz', (180 * avgMichalSize), 3600), ('*[3-4][0-8][0-9].dat.gz', 'file_180_B.dat.gz', (180 * avgMichalSize), 3600), ('*[3-4][0-9][0-9].dat.gz', 'file_200_A.dat.gz', (200 * avgMichalSize), 3600), ('*[3-4][0-9][0-9].dat.gz', 'file_200_B.dat.gz', (200 * avgMichalSize), 3600), ('*[3-5][0-9][0-9].dat.gz', 'file_300.dat.gz', (300 * avgMichalSize), 3600), ('*[3-5][0-9][0-9].dat.gz', 'file_300.dat.gz', (300 * avgMichalSize), 3600)]
    if (1 == 0):
        bucket = 'home-0xdiag-datasets'
        importFolderPath = 'manyfiles-nflx-gz'
        print "Using .gz'ed files in", importFolderPath
        csvFilenameAll = [('*_[123][0-9][0-9]*.dat.gz', 'file_300.dat.gz', (300 * avgMichalSize), 3600), ('*_[1][5-9][0-9]*.dat.gz', 'file_100.dat.gz', (50 * avgMichalSize), 3600)]
    if (1 == 0):
        bucket = 'home-0xdiag-datasets'
        importFolderPath = 'standard'
        print "Using .gz'ed files in", importFolderPath
        csvFilenameAll = [('manyfiles-nflx-gz/file_1.dat.gz', 'file_1.dat.gz', (1 * avgMichalSize), 700), ('manyfiles-nflx-gz/file_[2][0-9].dat.gz', 'file_10.dat.gz', (10 * avgMichalSize), 700), ('manyfiles-nflx-gz/file_[34][0-9].dat.gz', 'file_20.dat.gz', (20 * avgMichalSize), 700), ('manyfiles-nflx-gz/file_[5-9][0-9].dat.gz', 'file_50.dat.gz', (50 * avgMichalSize), 700), ('manyfiles-nflx-gz/file_1[0-9][0-9].dat.gz', 'file_100.dat.gz', (50 * avgMichalSize), 700), ('manyfiles-nflx-gz/file_[12][0-9][0-9].dat.gz', 'file_200.dat.gz', (50 * avgMichalSize), 700), ('manyfiles-nflx-gz/file_[12]?[0-9][0-9].dat.gz', 'file_300.dat.gz', (50 * avgMichalSize), 700), ('manyfiles-nflx-gz/file_*.dat.gz', 'file_384.dat.gz', (100 * avgMichalSize), 1200), ('covtype200x.data', 'covtype200x.data', covtype200xSize, 700)]
    csvFilenameList = csvFilenameAll
    trialMax = 1
    tryHeap = 28
    DO_GLM = False
    noPoll = False
    benchmarkLogging = ['cpu', 'disk', 'network', 'iostats', 'jstack']
    benchmarkLogging = ['cpu', 'disk', 'network', 'iostats']
    benchmarkLogging = ['cpu', 'disk', 'network']
    pollTimeoutSecs = 120
    retryDelaySecs = 10
    jea = ('-XX:MaxDirectMemorySize=512m -XX:+PrintGCDetails' + ' -Dh2o.find-ByteBuffer-leaks')
    jea = '-XX:MaxDirectMemorySize=512m -XX:+PrintGCDetails'
    jea = '-XX:+UseParNewGC -XX:+UseConcMarkSweepGC'
    jea = ((((' -Dcom.sun.management.jmxremote.port=54330' + ' -Dcom.sun.management.jmxremote.authenticate=false') + ' -Dcom.sun.management.jmxremote.ssl=false') + ' -Dcom.sun.management.jmxremote') + ' -Dcom.sun.management.jmxremote.local.only=false')
    jea = ' -Dlog.printAll=true'
    for (i, (csvFilepattern, csvFilename, totalBytes, timeoutSecs)) in enumerate(csvFilenameList):
        localhost = h2o.decide_if_localhost()
        if localhost:
            h2o.build_cloud(2, java_heap_GB=tryHeap, enable_benchmark_log=True)
        else:
            h2o_hosts.build_cloud_with_hosts(enable_benchmark_log=True)
        for trial in range(trialMax):
            csvPathname = ((importFolderPath + '/') + csvFilepattern)
            (importResult, importPattern) = h2i.import_only(bucket=bucket, path=csvPathname, schema='local')
            importFullList = importResult['files']
            importFailList = importResult['fails']
            print '\n Problem if this is not empty: importFailList:', h2o.dump_json(importFailList)
            h2o.cloudPerfH2O.change_logfile(csvFilename)
            h2o.cloudPerfH2O.message('')
            h2o.cloudPerfH2O.message((('Parse ' + csvFilename) + ' Start--------------------------------'))
            csvPathname = ((importFolderPath + '/') + csvFilepattern)
            start = time.time()
            parseResult = h2i.import_parse(bucket=bucket, path=csvPathname, schema='local', hex_key=(csvFilename + '.hex'), timeoutSecs=timeoutSecs, retryDelaySecs=retryDelaySecs, pollTimeoutSecs=pollTimeoutSecs, noPoll=noPoll, benchmarkLogging=benchmarkLogging)
            if noPoll:
                if ((i + 1) < len(csvFilenameList)):
                    time.sleep(1)
                    h2o.check_sandbox_for_errors()
                    (csvFilepattern, csvFilename, totalBytes2, timeoutSecs) = csvFilenameList[(i + 1)]
                    csvPathname = ((importFolderPath + '/') + csvFilepattern)
                    parseResult = h2i.import_parse(bucket=bucket, path=csvPathname, schema='local', hex_key=(csvFilename + '.hex'), timeoutSecs=timeoutSecs, retryDelaySecs=retryDelaySecs, pollTimeoutSecs=pollTimeoutSecs, noPoll=noPoll, benchmarkLogging=benchmarkLogging)
                if ((i + 2) < len(csvFilenameList)):
                    time.sleep(1)
                    h2o.check_sandbox_for_errors()
                    (csvFilepattern, csvFilename, totalBytes3, timeoutSecs) = csvFilenameList[(i + 2)]
                    csvPathname = ((importFolderPath + '/') + csvFilepattern)
                    parseResult = h2i.import_parse(bucket=bucket, path=csvPathname, schema='local', hex_key=(csvFilename + '.hex'), timeoutSecs=timeoutSecs, retryDelaySecs=retryDelaySecs, pollTimeoutSecs=pollTimeoutSecs, noPoll=noPoll, benchmarkLogging=benchmarkLogging)
            elapsed = (time.time() - start)
            print 'Parse #', trial, 'completed in', ('%6.2f' % elapsed), 'seconds.', ('%d pct. of timeout' % ((elapsed * 100) / timeoutSecs))
            if noPoll:
                time.sleep(2)
                h2o_jobs.pollWaitJobs(pattern=csvFilename, timeoutSecs=timeoutSecs, benchmarkLogging=benchmarkLogging)
                totalBytes += (totalBytes2 + totalBytes3)
                elapsed = (time.time() - start)
                h2o.check_sandbox_for_errors()
            if (totalBytes is not None):
                fileMBS = ((totalBytes / 1000000.0) / elapsed)
                l = '{!s} jvms, {!s}GB heap, {:s} {:s} {:6.2f} MB/sec for {:.2f} secs'.format(len(h2o.nodes), h2o.nodes[0].java_heap_GB, csvFilepattern, csvFilename, fileMBS, elapsed)
                print l
                h2o.cloudPerfH2O.message(l)
            print "Parse result['destination_key']:", parseResult['destination_key']
            if (not noPoll):
                h2o_cmd.columnInfoFromInspect(parseResult['destination_key'], exceptionOnMissingValues=False)
            origKey = parseResult['destination_key']
            execExpr = ('a = %s[1:200,]' % origKey)
            h2e.exec_expr(h2o.nodes[0], execExpr, 'a', timeoutSecs=30)
            newParseKey = {'destination_key': 'a', }
            print ('\n' + csvFilepattern)
            print 'Temporarily hacking to do nothing instead of RF on the parsed file'
            if DO_GLM:
                x = range(542)
                for i in [3, 4, 5, 6, 7, 8, 9, 10, 11, 14, 16, 17, 18, 19, 20, 424, 425, 426, 540, 541, 378]:
                    x.remove(i)
                x = ','.join(map(str, x))
                GLMkwargs = {'x': x, 'y': 378, 'case': 15, 'case_mode': '>', 'max_iter': 10, 'n_folds': 1, 'alpha': 0.2, 'lambda': 1e-05, }
                start = time.time()
                glm = h2o_cmd.runGLM(parseResult=parseResult, timeoutSecs=timeoutSecs, **GLMkwargs)
                h2o_glm.simpleCheckGLM(self, glm, None, **GLMkwargs)
                elapsed = (time.time() - start)
                h2o.check_sandbox_for_errors()
                l = '{:d} jvms, {:d}GB heap, {:s} {:s} GLM: {:6.2f} secs'.format(len(h2o.nodes), h2o.nodes[0].java_heap_GB, csvFilepattern, csvFilename, elapsed)
                print l
                h2o.cloudPerfH2O.message(l)
            h2o_cmd.checkKeyDistribution()
            h2i.delete_keys_from_import_result(pattern=csvFilename, importResult=importResult)
            h2o.tear_down_cloud()
            if (not localhost):
                print 'Waiting 30 secs before building cloud again (sticky ports?)'
            sys.stdout.write('.')
            sys.stdout.flush()
