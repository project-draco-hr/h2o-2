def test_GBM_manyfiles_multijob(self):
    bucket = 'home-0xdiag-datasets'
    modelKey = 'GBMModelKey'
    if h2o.localhost:
        files = [('manyfiles-nflx-gz', 'file_1.dat.gz', 'train.hex', 1800, None, 'file_1.dat.gz', 'test.hex')]
    else:
        files = [('manyfiles-nflx-gz', 'file_[0-9].dat.gz', 'train.hex', 1800, None, 'file_1[0-9].dat.gz', 'test.hex')]
    for (importFolderPath, trainFilename, trainKey, timeoutSecs, response, testFilename, testKey) in files:
        start = time.time()
        xList = []
        eList = []
        fList = []
        csvPathname = ((importFolderPath + '/') + trainFilename)
        parseTrainResult = h2i.import_parse(bucket=bucket, path=csvPathname, schema='local', hex_key=trainKey, timeoutSecs=timeoutSecs, doSummary=False)
        elapsed = (time.time() - start)
        print 'train parse end on ', trainFilename, 'took', elapsed, 'seconds', ('%d pct. of timeout' % ((elapsed * 100) / timeoutSecs))
        print 'train parse result:', parseTrainResult['destination_key']
        inspect = h2o_cmd.runInspect(key=parseTrainResult['destination_key'])
        print ('\n' + csvPathname), '    numRows:', '{:,}'.format(inspect['numRows']), '    numCols:', '{:,}'.format(inspect['numCols'])
        numRows = inspect['numRows']
        numCols = inspect['numCols']
        execExpr = ('%s[,378+1]=%s[,378+1]>15' % (trainKey, trainKey))
        if (not DO_FAIL):
            execExpr += ('; factor(%s[, 378+1]);' % trainKey)
        resultExec = h2o_cmd.runExec(str=execExpr, timeoutSecs=180)
        csvPathname = ((importFolderPath + '/') + testFilename)
        parseTestResult = h2i.import_parse(bucket=bucket, path=csvPathname, schema='local', hex_key=testKey, timeoutSecs=timeoutSecs, doSummary=False)
        elapsed = (time.time() - start)
        print 'test parse end on ', testFilename, 'took', elapsed, 'seconds', ('%d pct. of timeout' % ((elapsed * 100) / timeoutSecs))
        print 'test parse result:', parseTestResult['destination_key']
        execExpr = ('%s[,378+1]=%s[,378+1]>15' % (testKey, testKey))
        if (not DO_FAIL):
            execExpr += ('; factor(%s[, 378+1]);' % testKey)
        resultExec = h2o_cmd.runExec(str=execExpr, timeoutSecs=180)
        response = 378
        x = range(numCols)
        del x[response]
        ignored_cols_by_name = ','.join(map((lambda x: ('C' + str((x + 1)))), random.sample(x, 300)))
        print (('Using the same response %s for train and test (which should have a output value too)' % 'C') + str((response + 1)))
        ntrees = 10
        trial = 0
        print 'Kicking off multiple GBM jobs at once'
        if DO_FAIL:
            cases = [5, 10, 20, 40]
        else:
            cases = [5, 10, 20]
        for max_depth in cases:
            trial += 1
            params = {'response': ('C' + str((response + 1))), 'learn_rate': 0.2, 'nbins': 1024, 'ntrees': ntrees, 'max_depth': max_depth, 'min_rows': 10, 'validation': parseTestResult['destination_key'], 'ignored_cols_by_name': ignored_cols_by_name, 'grid_parallelism': 1, 'classification': (1 if DO_CLASSIFICATION else 0), }
            kwargs = params.copy()
            trainStart = time.time()
            gbmTrainResult = h2o_cmd.runGBM(parseResult=parseTrainResult, noPoll=True, timeoutSecs=(timeoutSecs * 4), destination_key=((modelKey + '_') + str(trial)), **kwargs)
            trainElapsed = (time.time() - trainStart)
            print 'GBM dispatch completed in', trainElapsed, 'seconds. On dataset: ', trainFilename
        statMean = h2j.pollStatsWhileBusy(timeoutSecs=timeoutSecs, pollTimeoutSecs=timeoutSecs, retryDelaySecs=5)
        num_cpus = (statMean['num_cpus'],)
        my_cpu_pct = (statMean['my_cpu_%'],)
        sys_cpu_pct = (statMean['sys_cpu_%'],)
        system_load = statMean['system_load']
        h2j.pollWaitJobs(timeoutSecs=timeoutSecs, pollTimeoutSecs=timeoutSecs)
