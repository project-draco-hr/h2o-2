def test_benchmark_import(self):
    avgMichalSizeUncompressed = 237270000
    avgMichalSize = 116561140
    avgSynSize = 4020000
    covtype200xSize = 15033863400
    if (1 == 0):
        importFolderPath = '/home2/0xdiag/datasets'
        print "Using non-.gz'ed files in", importFolderPath
        csvFilenameAll = [('manyfiles-nflx/file_1.dat', 'file_1.dat', (1 * avgMichalSizeUncompressed), 700), ('manyfiles-nflx/file_[2][0-9].dat', 'file_10.dat', (10 * avgMichalSizeUncompressed), 700), ('manyfiles-nflx/file_[34][0-9].dat', 'file_20.dat', (20 * avgMichalSizeUncompressed), 700), ('manyfiles-nflx/file_[5-9][0-9].dat', 'file_50.dat', (50 * avgMichalSizeUncompressed), 700), ('manyfiles-nflx/file_[0-9][0-9]*.dat', 'file_100.dat', (100 * avgMichalSizeUncompressed), 700), ('onefile-nflx/file_1_to_100.dat', 'file_single.dat', (100 * avgMichalSizeUncompressed), 1200)]
    if (1 == 1):
        importFolderPath = '/home/0xdiag/datasets'
        print "Using .gz'ed files in", importFolderPath
        csvFilenameAll = [('covtype200x.data', 'covtype200x.data', covtype200xSize, 700), ('manyfiles-nflx-gz/file_1.dat.gz', 'file_1.dat.gz', (1 * avgMichalSize), 700), ('manyfiles-nflx-gz/file_[2][0-9].dat.gz', 'file_10.dat.gz', (10 * avgMichalSize), 700), ('manyfiles-nflx-gz/file_[34][0-9].dat.gz', 'file_20.dat.gz', (20 * avgMichalSize), 700), ('manyfiles-nflx-gz/file_[5-9][0-9].dat.gz', 'file_50.dat.gz', (50 * avgMichalSize), 700), ('manyfiles-nflx-gz/file_*.dat.gz', 'file_100.dat.gz', (100 * avgMichalSize), 1200)]
    csvFilenameList = csvFilenameAll
    trialMax = 1
    base_port = 54321
    tryHeap = 28
    noPoll = False
    benchmarkLogging = ['cpu', 'disk']
    pollTimeoutSecs = 120
    retryDelaySecs = 10
    for (csvFilepattern, csvFilename, totalBytes, timeoutSecs) in csvFilenameList:
        localhost = h2o.decide_if_localhost()
        if localhost:
            h2o.build_cloud(1, java_heap_GB=tryHeap, base_port=base_port, enable_benchmark_log=True)
        else:
            h2o_hosts.build_cloud_with_hosts(1, java_heap_GB=tryHeap, base_port=base_port, enable_benchmark_log=True)
        for trial in range(trialMax):
            importFolderResult = h2i.setupImportFolder(None, importFolderPath)
            importFullList = importFolderResult['succeeded']
            importFailList = importFolderResult['failed']
            print '\n Problem if this is not empty: importFailList:', h2o.dump_json(importFailList)
            h2o.cloudPerfH2O.change_logfile(csvFilename)
            h2o.cloudPerfH2O.message('')
            h2o.cloudPerfH2O.message((('Parse ' + csvFilename) + ' Start--------------------------------'))
            start = time.time()
            parseKey = h2i.parseImportFolderFile(None, csvFilepattern, importFolderPath, key2=(csvFilename + '.hex'), timeoutSecs=timeoutSecs, retryDelaySecs=retryDelaySecs, pollTimeoutSecs=pollTimeoutSecs, noPoll=noPoll, benchmarkLogging=benchmarkLogging)
            if noPoll:
                if ((i + 1) < len(csvFilenameList)):
                    time.sleep(1)
                    h2o.check_sandbox_for_errors()
                    (csvFilepattern, csvFilename, totalBytes2, timeoutSecs) = csvFilenameList[(i + 1)]
                    s3nKey = ((URI + '/') + csvFilepattern)
                    key2 = (((csvFilename + '_') + str(trial)) + '.hex')
                    print 'Loading', protocol, 'key:', s3nKey, 'to', key2
                    parse2Key = h2o.nodes[0].parse(s3nKey, key2, timeoutSecs=timeoutSecs, retryDelaySecs=retryDelaySecs, pollTimeoutSecs=pollTimeoutSecs, noPoll=noPoll, benchmarkLogging=benchmarkLogging)
                if ((i + 2) < len(csvFilenameList)):
                    time.sleep(1)
                    h2o.check_sandbox_for_errors()
                    (csvFilepattern, csvFilename, totalBytes3, timeoutSecs) = csvFilenameList[(i + 2)]
                    s3nKey = ((URI + '/') + csvFilepattern)
                    key2 = (((csvFilename + '_') + str(trial)) + '.hex')
                    print 'Loading', protocol, 'key:', s3nKey, 'to', key2
                    parse3Key = h2o.nodes[0].parse(s3nKey, key2, timeoutSecs=timeoutSecs, retryDelaySecs=retryDelaySecs, pollTimeoutSecs=pollTimeoutSecs, noPoll=noPoll, benchmarkLogging=benchmarkLogging)
            elapsed = (time.time() - start)
            print 'Parse #', trial, 'completed in', ('%6.2f' % elapsed), 'seconds.', ('%d pct. of timeout' % ((elapsed * 100) / timeoutSecs))
            if noPoll:
                time.sleep(2)
                h2o_jobs.pollWaitJobs(pattern=csvFilename, timeoutSecs=timeoutSecs, benchmarkLogging=benchmarkLogging)
                totalBytes += (totalBytes2 + totalBytes3)
                elapsed = (time.time() - start)
                h2o.check_sandbox_for_errors()
            if (totalBytes is not None):
                fileMBS = ((totalBytes / 1000000.0) / elapsed)
                l = '{!s} jvms, {!s}GB heap, {:s} {:s} {:6.2f} MB/sec for {:.2f} secs'.format(len(h2o.nodes), tryHeap, csvFilepattern, csvFilename, fileMBS, elapsed)
                print l
                h2o.cloudPerfH2O.message(l)
            print csvFilepattern, 'parse time:', parseKey['response']['time']
            print "Parse result['destination_key']:", parseKey['destination_key']
            if (not noPoll):
                h2o_cmd.check_enums_from_inspect(parseKey)
            origKey = parseKey['destination_key']
            execExpr = (('a = slice(' + origKey) + ',1,200)')
            h2e.exec_expr(h2o.nodes[0], execExpr, 'a', timeoutSecs=30)
            newParseKey = {'destination_key': 'a', }
            print ('\n' + csvFilepattern)
            print 'Temporarily hacking to do nothing instead of RF on the parsed file'
            h2o_cmd.check_key_distribution()
            h2o_cmd.delete_csv_key(csvFilename, importFullList)
            h2o.tear_down_cloud()
            if (not localhost):
                print 'Waiting 30 secs before building cloud again (sticky ports?)'
                time.sleep(30)
            sys.stdout.write('.')
            sys.stdout.flush()
