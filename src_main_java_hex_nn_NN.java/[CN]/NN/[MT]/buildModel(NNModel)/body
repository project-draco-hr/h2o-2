{
  logStart();
  lock();
  Log.info("Number of chunks of the training data: " + source.anyVec().nChunks());
  if (validation != null)   Log.info("Number of chunks of the validation data: " + validation.anyVec().nChunks());
  if (model == null)   model=UKV.get(dest());
  final long model_size=model.model_info().size();
  Log.info("Number of model parameters (weights/biases): " + String.format("%,d",model_size));
  Log.info("Memory usage of the model: " + String.format("%.2f",(double)model_size * Float.SIZE / (1 << 23)) + " MB.");
  model.delete_and_lock(self());
  final Frame[] valid_adapted=validation == null ? null : model.adapt(validation,false);
  Frame train=_dinfo._adaptedFrame;
  Frame valid=validation == null ? null : valid_adapted[0];
  Frame trainScoreFrame=sampleFrame(train,score_training_samples,seed);
  Frame validScoreFrame=sampleFrame(valid,score_validation_samples,seed + 1);
  if (sync_samples > train.numRows()) {
    Log.warn("Setting sync_samples (" + sync_samples + ") to the number of rows of the training data ("+ (sync_samples=train.numRows())+ ").");
  }
  final float sync_fraction=sync_samples == 0l ? 1.0f : (float)sync_samples / train.numRows();
  Log.info("Starting to train the Neural Net model.");
  long timeStart=System.currentTimeMillis();
  do {
    NNTask nntask=new NNTask(_dinfo,model.model_info(),true,sync_fraction,shuffle_training_data).doAll(train);
    model.set_model_info(nntask.model_info());
  }
 while (model.doDiagnostics(trainScoreFrame,validScoreFrame,timeStart,self()));
  model.unlock(self());
  if (validScoreFrame != null && validScoreFrame != valid)   validScoreFrame.delete();
  if (trainScoreFrame != null && trainScoreFrame != train)   trainScoreFrame.delete();
  if (validation != null)   valid_adapted[1].delete();
  unlock();
  delete();
  Log.info("Finished training the Neural Net model.");
  return model;
}
