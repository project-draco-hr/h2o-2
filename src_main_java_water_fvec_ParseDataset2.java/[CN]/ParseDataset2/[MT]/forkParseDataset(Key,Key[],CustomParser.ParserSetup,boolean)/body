{
  setup.checkDupColumnNames();
  long sum=0;
  for (  Key k : keys) {
    if (dest.equals(k))     throw new IllegalArgumentException("Destination key " + dest + " must be different from all sources");
    sum+=DKV.get(k).length();
  }
  long memsz=0;
  for (  H2ONode h2o : H2O.CLOUD._memary)   memsz+=h2o.get_max_mem();
  if (sum > memsz * 4)   throw new IllegalArgumentException("Total input file size of " + PrettyPrint.bytes(sum) + " is much larger than total cluster memory of "+ PrettyPrint.bytes(memsz)+ ", please use either a larger cluster or smaller data.");
  ParseDataset2 job=new ParseDataset2(dest,keys);
  new Frame(job.dest(),new String[0],new Vec[0]).delete_and_lock(job.self());
  for (  Key k : keys)   Lockable.read_lock(k,job.self());
  ParserFJTask fjt=new ParserFJTask(job,keys,setup,delete_on_done);
  H2OCountedCompleter cleanup=new H2OCountedCompleter(){
    @Override public void compute2(){
      System.out.println("cleanup compute2");
    }
    @Override public void onCompletion(    CountedCompleter caller){
      System.out.println("cleanup completion");
    }
    @Override public boolean onExceptionalCompletion(    Throwable ex,    CountedCompleter caller){
      System.out.println("parser canceled, removing keys1");
      for (      Key k : keys)       UKV.remove(k);
      System.out.println("parser canceled, removing keys2");
      return true;
    }
  }
;
  fjt.setCompleter(cleanup);
  job.start(cleanup);
  H2O.submitTask(fjt);
  return job;
}
