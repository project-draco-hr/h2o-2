def test_parse_multi_header_rand_fvec(self):
    h2o.beta_features = True
    SYNDATASETS_DIR = h2o.make_syn_dir()
    csvFilename = 'syn_ints.csv'
    csvPathname = ((SYNDATASETS_DIR + '/') + csvFilename)
    allowedLetters = 'abcdeABCDE01234[]'
    headerChoices = []
    for n in range(500):
        done = False
        while (not done):
            l = random.randint(1, 64)
            headerName = ''.join([random.choice(allowedLetters) for _ in range(l)])
            done = (headerName not in headerChoices)
        headerChoices.append(headerName)
    tryList = [(3, 5, 9, 'cA', 60, 0), (3, 5, 25, 'cA', 60, 0), (10, 100, 500, 'cA', 60, 0)]
    for trial in range(20):
        (fileNum, rowCount, colCount, hex_key, timeoutSecs, dataRowsWithHeader) = random.choice(tryList)
        print fileNum, rowCount, colCount, hex_key, timeoutSecs, dataRowsWithHeader
        print 'Wait while', fileNum, 'synthetic files are created in', SYNDATASETS_DIR
        rowxcol = ((str(rowCount) + 'x') + str(colCount))
        totalCols = (colCount + 1)
        totalDataRows = 0
        totalHeaderRows = 0
        HEADER_HAS_HDR_ROW = 1
        DATA_HAS_HDR_ROW = random.randint(0, 1)
        PARSE_PATTERN_INCLUDES_HEADER = random.randint(0, 1)
        DATA_FIRST_IS_COMMENT = 0
        HEADER_FIRST_IS_COMMENT = 0
        GZIP_DATA = random.randint(0, 1)
        GZIP_HEADER = random.randint(0, 1)
        SEP_CHAR_GEN = random.choice(paramsDict['separator'])
        HEADER_SEP_CHAR_GEN = random.choice(paramsDict['hdr_separator'])
        if (HEADER_SEP_CHAR_GEN == 'same'):
            HEADER_SEP_CHAR_GEN = SEP_CHAR_GEN
        if (DATA_HAS_HDR_ROW and HEADER_HAS_HDR_ROW and (SEP_CHAR_GEN != HEADER_SEP_CHAR_GEN)):
            HEADER_SEP_CHAR_GEN = SEP_CHAR_GEN
        if (HEADER_SEP_CHAR_GEN in (',', ' ')):
            if random.randint(0, 1):
                HEADER_SEP_CHAR_GEN = (' ' + HEADER_SEP_CHAR_GEN)
            if random.randint(0, 1):
                HEADER_SEP_CHAR_GEN = (HEADER_SEP_CHAR_GEN + ' ')
        kwargs = {}
        for (k, v) in paramsDict.items():
            kwargs[k] = random.choice(v)
        kwargs['separator'] = SEP_CHAR_GEN
        if ((SEP_CHAR_GEN == ' ') or (SEP_CHAR_GEN == ',')):
            del kwargs['separator']
        else:
            kwargs['separator'] = ord(SEP_CHAR_GEN)
        if (SEP_CHAR_GEN in (',', ' ')):
            if random.randint(0, 1):
                SEP_CHAR_GEN = (' ' + SEP_CHAR_GEN)
            if random.randint(0, 1):
                SEP_CHAR_GEN = (SEP_CHAR_GEN + ' ')
        print '\nHEADER_HAS_HDR_ROW:', HEADER_HAS_HDR_ROW
        print 'DATA_HAS_HDR_ROW:', DATA_HAS_HDR_ROW
        print 'PARSE_PATTERN_INCLUDES_HEADER', PARSE_PATTERN_INCLUDES_HEADER
        print 'DATA_FIRST_IS_COMMENT:', DATA_FIRST_IS_COMMENT
        print 'HEADER_FIRST_IS_COMMENT:', HEADER_FIRST_IS_COMMENT
        print 'SEP_CHAR_GEN:', (('->' + SEP_CHAR_GEN) + '<-')
        print 'HEADER_SEP_CHAR_GEN:', (('->' + HEADER_SEP_CHAR_GEN) + '<-')
        print 'GZIP_DATA:', GZIP_DATA
        print 'GZIP_HEADER:', GZIP_HEADER
        hfhList = (random.sample(headerChoices, colCount) + ['output'])
        headerForHeader = HEADER_SEP_CHAR_GEN.join(hfhList)
        print 'headerForHeader:', headerForHeader
        hfdList = hfhList
        headerForData = SEP_CHAR_GEN.join(hfdList)
        for fileN in range(fileNum):
            csvFilenameSuffix = (((((((str(fileN) + '_') + str(SEED)) + '_') + str(trial)) + '_') + rowxcol) + '_csv')
            csvFilename = ('syn_data_' + csvFilenameSuffix)
            csvPathname = ((SYNDATASETS_DIR + '/') + csvFilename)
            rList = rand_rowData(colCount, sepChar=SEP_CHAR_GEN)
            (headerRowsDone, dataRowsDone) = write_syn_dataset(csvPathname, rowCount, headerString=(headerForData if DATA_HAS_HDR_ROW else None), rList=rList, commentFirst=DATA_FIRST_IS_COMMENT, sepChar=SEP_CHAR_GEN)
            totalDataRows += dataRowsDone
            totalHeaderRows += headerRowsDone
            if GZIP_DATA:
                csvPathnamegz = (csvPathname + '.gz')
                print 'gzipping to', csvPathnamegz
                h2o_util.file_gzip(csvPathname, csvPathnamegz)
                os.rename(csvPathname, ((SYNDATASETS_DIR + '/not_used_data_') + csvFilenameSuffix))
        hdrFilenameSuffix = (((((str(SEED) + '_') + str(trial)) + '_') + rowxcol) + '_csv')
        hdrFilename = ('syn_header_' + hdrFilenameSuffix)
        hdrPathname = ((SYNDATASETS_DIR + '/') + hdrFilename)
        (headerRowsDone, dataRowsDone) = write_syn_dataset(hdrPathname, dataRowsWithHeader, headerString=(headerForHeader if HEADER_HAS_HDR_ROW else None), rList=rList, commentFirst=HEADER_FIRST_IS_COMMENT, sepChar=SEP_CHAR_GEN)
        if PARSE_PATTERN_INCLUDES_HEADER:
            totalDataRows += dataRowsDone
        totalHeaderRows += headerRowsDone
        if GZIP_HEADER:
            hdrPathnamegz = (hdrPathname + '.gz')
            print 'gzipping to', hdrPathnamegz
            h2o_util.file_gzip(hdrPathname, hdrPathnamegz)
            os.rename(hdrPathname, ((SYNDATASETS_DIR + '/not_used_header_') + hdrFilenameSuffix))
        hex_key = (('syn_dst' + str(trial)) + '.hex')
        fileList = os.listdir(SYNDATASETS_DIR)
        for f in fileList:
            h2i.import_only(path=((SYNDATASETS_DIR + '/') + f), schema='put', noPrint=True)
        h2o_cmd.runStoreView()
        headerKey = h2i.find_key(hdrFilename)
        dataKey = h2i.find_key(csvFilename)
        print 'Header Key =', headerKey
        if (kwargs['header_from_file'] == 'header'):
            kwargs['header_from_file'] = headerKey
        elif (kwargs['header_from_file'] == 'data'):
            kwargs['header_from_file'] = dataKey
        if (not HEADER_HAS_HDR_ROW):
            kwargs['header_from_file'] = None
        if (HEADER_HAS_HDR_ROW and (kwargs['header_from_file'] == headerKey)):
            ignoreForRf = hfhList[0]
        elif DATA_HAS_HDR_ROW:
            ignoreForRf = hfdList[0]
        else:
            ignoreForRf = None
        print 'If header_from_file= , required to force header=1 for h2o'
        if kwargs['header_from_file']:
            kwargs['header'] = 1
        elif DATA_HAS_HDR_ROW:
            kwargs['header'] = 1
        else:
            kwargs['header'] = 0
        start = time.time()
        if (PARSE_PATTERN_INCLUDES_HEADER and HEADER_HAS_HDR_ROW):
            pattern = (((('syn_*' + str(trial)) + '_') + rowxcol) + '*')
        else:
            pattern = (((('syn_data_*' + str(trial)) + '_') + rowxcol) + '*')
        kwargs.pop('hdr_separator', None)
        parseResult = h2i.parse_only(pattern=pattern, hex_key=hex_key, timeoutSecs=timeoutSecs, **kwargs)
        print ("parseResult['destination_key']: " + parseResult['destination_key'])
        inspect = h2o_cmd.runInspect(None, parseResult['destination_key'])
        h2o_cmd.infoFromInspect(inspect, csvPathname)
        print ('\n' + csvPathname), '    numRows:', '{:,}'.format(inspect['numRows']), '    numCols:', '{:,}'.format(inspect['numCols'])
        h2o_cmd.columnInfoFromInspect(parseResult['destination_key'], exceptionOnMissingValues=False)
        self.assertEqual(inspect['numCols'], totalCols, ('parse created result with the wrong number of cols %s %s' % (inspect['numCols'], totalCols)))
        h2oLosesOneData = ((headerRowsDone == 0) and (kwargs['header'] == 1) and (not DATA_HAS_HDR_ROW))
        h2oGainsOneData = ((headerRowsDone != 0) and (kwargs['header'] == 1) and DATA_HAS_HDR_ROW and (kwargs['header_from_file'] is not None))
        h2oGainsOneData = False
        print 'h2oLosesOneData:', h2oLosesOneData
        print 'h2oGainsOneData:', h2oGainsOneData
        if h2oLosesOneData:
            totalDataRows -= 1
        if h2oGainsOneData:
            totalDataRows += 1
        if (1 == 0):
            self.assertEqual(inspect['numRows'], totalDataRows, ("parse created result with the wrong number of rows h2o %s gen'ed: %s" % (inspect['numRows'], totalDataRows)))
        kwargs = {'sample': 100, 'depth': 25, 'ntree': 2, 'ignore': ignoreForRf, }
        start = time.time()
        elapsed = (time.time() - start)
        print ('%d pct. of timeout' % ((elapsed / timeoutSecs) * 100))
        print 'trial #', trial, 'totalDataRows:', totalDataRows, 'parse end on ', csvFilename, 'took', (time.time() - start), 'seconds'
        h2o.check_sandbox_for_errors()
        h2i.delete_keys_at_all_nodes(pattern='syn_datasets')
