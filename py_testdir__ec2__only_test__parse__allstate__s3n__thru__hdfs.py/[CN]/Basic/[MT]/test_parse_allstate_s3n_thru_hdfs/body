def test_parse_allstate_s3n_thru_hdfs(self):
    csvFilename = 'train_set.csv'
    csvPathname = ('allstate/' + csvFilename)
    URI = 's3n://home-0xdiag-datasets/'
    s3nKey = (URI + csvPathname)
    trialMax = 3
    for trial in range(trialMax):
        trialStart = time.time()
        importHDFSResult = h2o.nodes[0].import_hdfs(URI)
        s3nFullList = importHDFSResult['succeeded']
        self.assertGreater(len(s3nFullList), 1, "Didn't see more than 1 files in s3n?")
        storeView = h2o.nodes[0].store_view()
        for s in storeView['keys']:
            print '\nkey:', s['key']
            if ('rows' in s):
                print 'rows:', s['rows'], 'value_size_bytes:', s['value_size_bytes']
        key2 = (((csvFilename + '_') + str(trial)) + '.hex')
        print 'Loading s3n key: ', s3nKey, 'thru HDFS'
        timeoutSecs = 500
        start = time.time()
        parseResult = h2o.nodes[0].parse(s3nKey, key2, timeoutSecs=timeoutSecs, retryDelaySecs=10, pollTimeoutSecs=60, noise=('JStack', none))
        elapsed = (time.time() - start)
        print s3nKey, 'h2o reported parse time:', parseResult['response']['time']
        print 'parse end on ', s3nKey, 'took', elapsed, 'seconds', ('%d pct. of timeout' % ((elapsed * 100) / timeoutSecs))
        print 'parse result:', parseResult['destination_key']
        print 'Deleting key in H2O so we get it from S3 (if ec2) or nfs again.', 'Otherwise it would just parse the cached key.'
        storeView = h2o.nodes[0].store_view()
        storeView = h2o.nodes[0].store_view()
        for s in storeView['keys']:
            print '\nkey:', s['key']
            if ('rows' in s):
                print 'rows:', s['rows'], 'value_size_bytes:', s['value_size_bytes']
        print 'Trial #', trial, 'completed in', (time.time() - trialStart), 'seconds.',
