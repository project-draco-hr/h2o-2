{
  Vec[] vecs=fr.vecs();
  Vec c=vecs[vecs.length - 1];
  if (!_params.regression) {
    final int classes=c.cardinality();
    if (!(2 <= classes && classes <= 254))     throw new IllegalArgumentException("Response contains " + classes + " classes, but algorithm supports only 254 levels");
  }
  if (_params.num_split_features != -1 && (_params.num_split_features < 1 || _params.num_split_features > vecs.length - 1))   throw new IllegalArgumentException("Number of split features exceeds available data. Should be in [1," + (vecs.length - 1) + "]");
  ChunkAllocInfo cai=new ChunkAllocInfo();
  boolean can_load_all=canLoadAll(fr,cai);
  if (_params._useNonLocalData && !can_load_all) {
    String heap_warning="This algorithm requires loading of all data from remote nodes." + "\nThe node " + cai.node + " requires "+ PrettyPrint.bytes(cai.requiredMemory)+ " more memory to load all data and perform computation but there is only "+ PrettyPrint.bytes(cai.availableMemory)+ " of available memory."+ "\n\nPlease provide more memory for JVMs \n\n-OR-\n\n Try Big Data Random Forest: ";
    Log.warn(heap_warning);
    throw new IllegalArgumentException(heap_warning + DRF.link(Key.make(_key),"Big Data Random Forest"));
  }
  if (can_load_all) {
    _params._useNonLocalData=true;
    Log.info("Enough available free memory to compute on all data. Pulling all data locally and then launching RF.");
  }
}
