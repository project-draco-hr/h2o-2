{
  Frame validScoreFrame=null;
  Frame train, trainScoreFrame;
  try {
    lock_data();
    if (checkpoint == null)     logStart();
    if (model == null) {
      model=UKV.get(dest());
    }
    model.write_lock(self());
    prepareValidationWithModel(model);
    final long model_size=model.model_info().size();
    Log.info("Number of model parameters (weights/biases): " + String.format("%,d",model_size));
    train=model.model_info().data_info()._adaptedFrame;
    train=updateFrame(train,reBalance(train,seed,replicate_training_data));
    float[] trainSamplingFactors;
    if (classification && balance_classes) {
      trainSamplingFactors=new float[train.lastVec().domain().length];
      train=updateFrame(train,sampleFrameStratified(train,train.lastVec(),trainSamplingFactors,(long)(max_after_balance_size * train.numRows()),seed,true,false));
      model.setModelClassDistribution(new MRUtils.ClassDist(train.lastVec()).doAll(train.lastVec()).rel_dist());
    }
    model.training_rows=train.numRows();
    trainScoreFrame=sampleFrame(train,score_training_samples,seed);
    if (train != trainScoreFrame)     ltrash(trainScoreFrame);
    Log.info("Number of chunks of the training data: " + train.anyVec().nChunks());
    if (validation != null) {
      Frame adaptedValid=getValidation();
      if (getValidAdaptor().needsAdaptation2CM()) {
        adaptedValid.add(getValidAdaptor().adaptedValidationResponse(_responseName),getValidAdaptor().getAdaptedValidationResponse2CM());
      }
      if (classification && balance_classes && score_validation_sampling == ClassSamplingMethod.Stratified) {
        validScoreFrame=updateFrame(adaptedValid,sampleFrameStratified(adaptedValid,adaptedValid.lastVec(),null,score_validation_samples > 0 ? score_validation_samples : adaptedValid.numRows(),seed + 1,false,false));
      }
 else {
        validScoreFrame=updateFrame(adaptedValid,sampleFrame(adaptedValid,score_validation_samples,seed + 1));
      }
      validScoreFrame=updateFrame(validScoreFrame,reBalance(validScoreFrame,seed + 1,false));
      Log.info("Number of chunks of the validation data: " + validScoreFrame.anyVec().nChunks());
    }
    if ((mini_batch == -1 || mini_batch == 0 || mini_batch > train.numRows()) && !replicate_training_data) {
      Log.warn("Setting mini_batch (" + mini_batch + ") to one epoch: #rows ("+ (mini_batch=train.numRows())+ ").");
    }
    if ((mini_batch == -1 || mini_batch > H2O.CLOUD.size() * train.numRows()) && replicate_training_data) {
      Log.warn("Setting mini_batch (" + mini_batch + ") to the largest possible number: #nodes x #rows ("+ (mini_batch=H2O.CLOUD.size() * train.numRows())+ ").");
    }
    final float sync_fraction=mini_batch == 0l ? 1.0f : (float)mini_batch / train.numRows();
    if (!quiet_mode)     Log.info("Initial model:\n" + model.model_info());
    Log.info("Starting to train the Deep Learning model.");
    do     model.set_model_info(H2O.CLOUD.size() > 1 && replicate_training_data ? (single_node ? new DeepLearningTask2(train,model.model_info(),sync_fraction).invoke(Key.make()).model_info() : new DeepLearningTask2(train,model.model_info(),sync_fraction / H2O.CLOUD.size()).invokeOnAllNodes().model_info()) : new DeepLearningTask(model.model_info(),sync_fraction).doAll(train).model_info());
 while (model.doScoring(train,trainScoreFrame,validScoreFrame,self(),getValidAdaptor()));
    Log.info("Finished training the Deep Learning model.");
    return model;
  }
 catch (  JobCancelledException ex) {
    Log.info("Deep Learning model building was cancelled.");
    model=UKV.get(dest());
    return model;
  }
catch (  Exception ex) {
    ex.printStackTrace();
    throw new RuntimeException(ex);
  }
 finally {
    if (model != null)     model.unlock(self());
    unlock_data();
    emptyLTrash();
  }
}
