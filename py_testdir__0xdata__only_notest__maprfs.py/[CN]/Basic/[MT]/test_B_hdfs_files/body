def test_B_hdfs_files(self):
    print '\nLoad a list of files from HDFS, parse and do 1 RF tree'
    print '\nYou can try running as hduser/hduser if fail'
    csvFilenameAll = ['TEST-poker1000.csv', 'and-testing.data', 'arcene2_train.both', 'arcene_train.both', 'bestbuy_test.csv', 'bestbuy_train.csv', 'covtype.13x.data', 'covtype.13x.shuffle.data', 'covtype.4x.shuffle.data', 'covtype.data', 'covtype4x.shuffle.data', 'hhp.unbalanced.012.1x11.data.gz', 'hhp.unbalanced.012.data.gz', 'hhp.unbalanced.data.gz', 'hhp2.os.noisy.0_1.data', 'hhp2.os.noisy.9_4.data', 'hhp_9_14_12.data', 'leads.csv', 'prostate_long_1G.csv']
    if (1 == 0):
        csvFilenameList = random.sample(csvFilenameAll, 8)
    else:
        csvFilenameList = csvFilenameAll
    benchmarkLogging = ['cpu', 'disk', 'network', 'iostats', 'jstack']
    benchmarkLogging = ['cpu', 'disk', 'network', 'iostats']
    benchmarkLogging = ['cpu', 'disk', 'network']
    benchmarkLogging = []
    importFolderPath = 'datasets'
    trial = 0
    for csvFilename in csvFilenameList:
        csvPathname = ((importFolderPath + '/') + csvFilename)
        timeoutSecs = 1000
        (importResult, importPattern) = h2i.import_only(path=csvPathname, schema='maprfs', timeoutSecs=timeoutSecs)
        succeeded = importResult['succeeded']
        if (len(succeeded) < 1):
            raise Exception(('Should have imported at least 1 key for' % csvPathname))
        foundIt = None
        for f in succeeded:
            if (csvPathname in f['key']):
                foundIt = f
                break
        if foundIt:
            value_size_bytes = f['value_size_bytes']
        else:
            raise Exception(('Should have found %s in the imported keys for %s' % (importPattern, csvPathname)))
        totalBytes = value_size_bytes
        print 'Loading', csvFilename, 'from maprfs'
        start = time.time()
        parseResult = h2i.import_parse(path=csvPathname, schema='maprfs', timeoutSecs=timeoutSecs, doSummary=False, benchmarkLogging=benchmarkLogging)
        print 'parse result:', parseResult['destination_key']
        elapsed = (time.time() - start)
        fileMBS = ((totalBytes / 1000000.0) / elapsed)
        l = '{!s} jvms, {!s}GB heap, {:s} {:s} {:6.2f}MB {:6.2f} MB/sec for {:.2f} secs'.format(len(h2o.nodes), h2o.nodes[0].java_heap_GB, 'Parse', csvPathname, ((totalBytes + 0.0) / 1000000.0), fileMBS, elapsed)
        print ('\n' + l)
        h2o.cloudPerfH2O.message(l)
        if DO_RF:
            print ('\n' + csvFilename)
            start = time.time()
            kwargs = {'ntree': 1, }
            paramsString = json.dumps(kwargs)
            RFview = h2o_cmd.runRF(parseResult=parseResult, timeoutSecs=2000, benchmarkLogging=benchmarkLogging, **kwargs)
            elapsed = (time.time() - start)
            print 'rf end on ', csvPathname, 'took', elapsed, 'seconds.', ('%d pct. of timeout' % ((elapsed / timeoutSecs) * 100))
            l = '{!s} jvms, {!s}GB heap, {:s} {:s} {:s} for {:.2f} secs {:s}'.format(len(h2o.nodes), h2o.nodes[0].java_heap_GB, 'KMeans', ('trial ' + str(trial)), csvFilename, elapsed, paramsString)
            print l
            h2o.cloudPerfH2O.message(l)
        print "Deleting all keys, to make sure our parse times don't include spills"
        h2i.delete_keys_at_all_nodes()
        trial += 1
