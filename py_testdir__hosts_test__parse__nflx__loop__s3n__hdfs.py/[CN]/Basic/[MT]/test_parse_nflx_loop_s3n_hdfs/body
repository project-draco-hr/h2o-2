def test_parse_nflx_loop_s3n_hdfs(self):
    avgMichalSize = 116561140
    csvFilenameList = [('manyfiles-nflx-gz/file_*.dat.gz', 'file_100.dat.gz', (100 * avgMichalSize)), ('manyfiles-nflx-gz/file_1.dat.gz', 'file_1.dat.gz', (1 * avgMichalSize)), ('manyfiles-nflx-gz/file_[2][0-9].dat.gz', 'file_10.dat.gz', (10 * avgMichalSize)), ('manyfiles-nflx-gz/file_[34][0-9].dat.gz', 'file_20.dat.gz', (20 * avgMichalSize)), ('manyfiles-nflx-gz/file_[5-9][0-9].dat.gz', 'file_50.dat.gz', (50 * avgMichalSize))]
    print 'Using the -.gz files from s3'
    URI = 's3n://home-0xdiag-datasets'
    trialMax = 1
    for (csvFilepattern, csvFilename, totalBytes) in csvFilenameList:
        s3nKey = ((URI + '/') + csvFilepattern)
        for tryHeap in [28]:
            print '\n', tryHeap, 'GB heap, 1 jvm per host, import hdfs/s3n, then parse'
            h2o_hosts.build_cloud_with_hosts(node_count=1, java_heap_GB=tryHeap, hdfs_name_node='10.78.14.235:9000', hdfs_version='0.20.2')
            h2o.nodes[0].sandbox_ignore_errors = True
            timeoutSecs = 1800
            for trial in range(trialMax):
                importHDFSResult = h2o.nodes[0].import_hdfs(URI)
                s3nFullList = importHDFSResult['succeeded']
                for k in s3nFullList:
                    key = k['key']
                    if (('nflx' in key) and ('file_1.dat.gz' in key)):
                        print "example file we'll use:", key
                self.assertGreater(len(s3nFullList), 8, "Didn't see more than 8 files in s3n?")
                key2 = (((csvFilename + '_') + str(trial)) + '.hex')
                print 'Loading s3n key: ', s3nKey, 'thru HDFS'
                start = time.time()
                parseKey = h2o.nodes[0].parse(s3nKey, key2, timeoutSecs=timeoutSecs, retryDelaySecs=10, pollTimeoutSecs=60)
                elapsed = (time.time() - start)
                print s3nKey, 'parse time:', parseKey['response']['time']
                print 'parse result:', parseKey['destination_key']
                print 'Parse #', trial, 'completed in', ('%6.2f' % elapsed), 'seconds.', ('%d pct. of timeout' % ((elapsed * 100) / timeoutSecs))
                if (totalBytes is not None):
                    fileMBS = ((totalBytes / 1000000.0) / elapsed)
                    print '\nMB/sec (before uncompress)', ('%6.2f' % fileMBS)
                print 'Deleting key in H2O so we get it from S3 (if ec2) or nfs again.', 'Otherwise it would just parse the cached key.'
                storeView = h2o.nodes[0].store_view()
                for k in s3nFullList:
                    deleteKey = k['key']
                    if ((csvFilename in deleteKey) and (not ('.hex' in key))):
                        print 'Removing', deleteKey
                        removeKeyResult = h2o.nodes[0].remove_key(key=deleteKey)
            h2o.tear_down_cloud()
            time.sleep(5)
