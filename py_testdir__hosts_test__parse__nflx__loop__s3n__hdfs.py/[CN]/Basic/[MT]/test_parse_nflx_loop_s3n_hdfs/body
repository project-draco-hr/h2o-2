def test_parse_nflx_loop_s3n_hdfs(self):
    DO_GLM = False
    USE_HOME2 = False
    USE_S3 = False
    noPoll = False
    benchmarkLogging = ['jstack', 'iostats']
    benchmarkLogging = ['iostats']
    avgMichalSize = 116561140
    avgSynSize = 4020000
    synSize = 183
    if USE_HOME2:
        csvFilenameList = [('00[0-4][0-9]_syn.csv.gz', 'file_50.dat.gz', (50 * synSize), 700), ('[0][1][0-9][0-9]_.*', 'file_100.dat.gz', (100 * synSize), 700), ('[0][0-4][0-9][0-9]_.*', 'file_500.dat.gz', (500 * synSize), 700), ('[0][0-9][0-9][0-9]_.*', 'file_1000.dat.gz', (1000 * synSize), 700)]
    else:
        csvFilenameList = [('[A-D]-800-manyfiles-nflx-gz/file_[0-9]*.dat.gz', 'file_A_800_x55.dat.gz', (800 * (avgMichalSize / 2)), 7200), ('manyfiles-nflx-gz/file_[123][0-9][0-9].dat.gz', 'file_300_A.dat.gz', (300 * avgMichalSize), 3600), ('[A-D]-800-manyfiles-nflx-gz/file_[0-9]*.dat.gz', 'file_B_800_x55.dat.gz', (800 * (avgMichalSize / 2)), 7200), ('[A-D]-800-manyfiles-nflx-gz/file_[0-9]*.dat.gz', 'file_C_800_x55.dat.gz', (800 * (avgMichalSize / 2)), 7200), ('[A-D]-800-manyfiles-nflx-gz/file_[0-9]*.dat.gz', 'file_D_800_x55.dat.gz', (800 * (avgMichalSize / 2)), 7200), ('[A-D]-800-manyfiles-nflx-gz/file_[0-9]*.dat.gz', 'file_E_800_x55.dat.gz', (800 * (avgMichalSize / 2)), 7200), ('[A-D]-800-manyfiles-nflx-gz/file_[0-9]*.dat.gz', 'file_F_800_x55.dat.gz', (800 * (avgMichalSize / 2)), 7200), ('manyfiles-nflx-gz/file_[123][0-9][0-9].dat.gz', 'file_300_B.dat.gz', (300 * avgMichalSize), 3600), ('manyfiles-nflx-gz/file_[123][0-9][0-9].dat.gz', 'file_300_C.dat.gz', (300 * avgMichalSize), 3600), ('manyfiles-nflx-gz/file_1.dat.gz', 'file_1.dat.gz', (1 * avgMichalSize), 300), ('manyfiles-nflx-gz/file_[2][0-9].dat.gz', 'file_10.dat.gz', (10 * avgMichalSize), 700), ('manyfiles-nflx-gz/file_[34][0-9].dat.gz', 'file_20.dat.gz', (20 * avgMichalSize), 900), ('manyfiles-nflx-gz/file_[5-9][0-9].dat.gz', 'file_50_A.dat.gz', (50 * avgMichalSize), 3600), ('manyfiles-nflx-gz/file_1[0-4][0-9].dat.gz', 'file_50_B.dat.gz', (50 * avgMichalSize), 3600), ('manyfiles-nflx-gz/file_1[0-9][0-9].dat.gz', 'file_100_A.dat.gz', (100 * avgMichalSize), 3600), ('manyfiles-nflx-gz/file_2[0-9][0-9].dat.gz', 'file_100_B.dat.gz', (100 * avgMichalSize), 3600), ('manyfiles-nflx-gz/file_[12][0-9][0-9].dat.gz', 'file_200_A.dat.gz', (200 * avgMichalSize), 3600), ('manyfiles-nflx-gz/file_[12][0-9][0-9].dat.gz', 'file_200_B.dat.gz', (200 * avgMichalSize), 3600)]
    print 'Using the -.gz files from s3'
    if USE_HOME2:
        bucket = 'home2-0xdiag-datasets/1k_small_gz'
    else:
        bucket = 'home-0xdiag-datasets'
    if USE_S3:
        URI = ('s3://' + bucket)
        protocol = 's3'
    else:
        URI = ('s3n://' + bucket)
        protocol = 's3n/hdfs'
    trialMax = 1
    pollTimeoutSecs = 180
    retryDelaySecs = 10
    for (i, (csvFilepattern, csvFilename, totalBytes, timeoutSecs)) in enumerate(csvFilenameList):
        for tryHeap in [28]:
            print '\n', tryHeap, 'GB heap, 1 jvm per host, import', protocol, 'then parse'
            jea = '-XX:+UseParNewGC -XX:+UseConcMarkSweepGC'
            h2o_hosts.build_cloud_with_hosts(node_count=1, java_heap_GB=tryHeap, enable_benchmark_log=True, timeoutSecs=120, retryDelaySecs=10, hdfs_name_node='10.78.14.235:9000', hdfs_version='0.20.2')
            h2o.nodes[0].sandbox_ignore_errors = True
            for trial in range(trialMax):
                if USE_S3:
                    importResult = h2o.nodes[0].import_s3(bucket)
                else:
                    importResult = h2o.nodes[0].import_hdfs(URI)
                s3nFullList = importResult['succeeded']
                for k in s3nFullList:
                    key = k['key']
                    if (csvFilepattern in key):
                        print "example file we'll use:", key
                        break
                    else:
                        pass
                self.assertGreater(len(s3nFullList), 8, "Didn't see more than 8 files in s3n?")
                s3nKey = ((URI + '/') + csvFilepattern)
                key2 = (((csvFilename + '_') + str(trial)) + '.hex')
                print 'Loading', protocol, 'key:', s3nKey, 'to', key2
                start = time.time()
                parseKey = h2o.nodes[0].parse(s3nKey, key2, timeoutSecs=timeoutSecs, retryDelaySecs=retryDelaySecs, pollTimeoutSecs=pollTimeoutSecs, noPoll=noPoll, benchmarkLogging=benchmarkLogging)
                if noPoll:
                    if ((i + 1) < len(csvFilenameList)):
                        time.sleep(1)
                        h2o.check_sandbox_for_errors()
                        (csvFilepattern, csvFilename, totalBytes2, timeoutSecs) = csvFilenameList[(i + 1)]
                        s3nKey = ((URI + '/') + csvFilepattern)
                        key2 = (((csvFilename + '_') + str(trial)) + '.hex')
                        print 'Loading', protocol, 'key:', s3nKey, 'to', key2
                        parse2Key = h2o.nodes[0].parse(s3nKey, key2, timeoutSecs=timeoutSecs, retryDelaySecs=retryDelaySecs, pollTimeoutSecs=pollTimeoutSecs, noPoll=noPoll, benchmarkLogging=benchmarkLogging)
                    if ((i + 2) < len(csvFilenameList)):
                        time.sleep(1)
                        h2o.check_sandbox_for_errors()
                        (csvFilepattern, csvFilename, totalBytes3, timeoutSecs) = csvFilenameList[(i + 2)]
                        s3nKey = ((URI + '/') + csvFilepattern)
                        key2 = (((csvFilename + '_') + str(trial)) + '.hex')
                        print 'Loading', protocol, 'key:', s3nKey, 'to', key2
                        parse3Key = h2o.nodes[0].parse(s3nKey, key2, timeoutSecs=timeoutSecs, retryDelaySecs=retryDelaySecs, pollTimeoutSecs=pollTimeoutSecs, noPoll=noPoll, benchmarkLogging=benchmarkLogging)
                elapsed = (time.time() - start)
                print s3nKey, 'parse time:', parseKey['response']['time']
                print 'parse result:', parseKey['destination_key']
                print 'Parse #', trial, 'completed in', ('%6.2f' % elapsed), 'seconds.', ('%d pct. of timeout' % ((elapsed * 100) / timeoutSecs))
                if noPoll:
                    time.sleep(2)
                    h2o_jobs.pollWaitJobs(pattern=csvFilename, timeoutSecs=timeoutSecs, benchmarkLogging=benchmarkLogging)
                    totalBytes += (totalBytes2 + totalBytes3)
                    elapsed = (time.time() - start)
                    h2o.check_sandbox_for_errors()
                if (totalBytes is not None):
                    fileMBS = ((totalBytes / 1000000.0) / elapsed)
                    l = '{:d} jvms, {:d}GB heap, {:s} {:s} {:6.2f} MB/sec for {:6.2f} secs'.format(len(h2o.nodes), tryHeap, csvFilepattern, csvFilename, fileMBS, elapsed)
                    print l
                    h2o.cloudPerfH2O.message(l)
                if (not noPoll):
                    h2o_cmd.check_enums_from_inspect(parseKey)
                if DO_GLM:
                    x = range(542)
                    for i in [3, 4, 5, 6, 7, 8, 9, 10, 11, 14, 16, 17, 18, 19, 20, 424, 425, 426, 540, 541, 378]:
                        x.remove(i)
                    x = ','.join(map(str, x))
                    GLMkwargs = {'x': x, 'y': 378, 'case': 15, 'case_mode': '>', 'max_iter': 10, 'n_folds': 1, 'alpha': 0.2, 'lambda': 1e-05, }
                    start = time.time()
                    glm = h2o_cmd.runGLMOnly(parseKey=parseKey, timeoutSecs=timeoutSecs, benchmarkLogging=benchmarkLogging, **GLMkwargs)
                    h2o_glm.simpleCheckGLM(self, glm, None, **GLMkwargs)
                    elapsed = (time.time() - start)
                    h2o.check_sandbox_for_errors()
                    l = '{:d} jvms, {:d}GB heap, {:s} {:s} GLM: {:6.2f} secs'.format(len(h2o.nodes), tryHeap, csvFilepattern, csvFilename, elapsed)
                    print l
                    h2o.cloudPerfH2O.message(l)
                print 'Deleting key in H2O so we get it from S3 (if ec2) or nfs again.', 'Otherwise it would just parse the cached key.'
                h2o_cmd.check_key_distribution()
                h2o_cmd.delete_csv_key(csvFilename, s3nFullList)
            h2o.tear_down_cloud()
            print 'Waiting 30 secs before building cloud again (sticky ports?)'
            time.sleep(30)
