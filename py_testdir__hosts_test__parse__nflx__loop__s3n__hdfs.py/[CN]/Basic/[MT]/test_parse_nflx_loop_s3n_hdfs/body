def test_parse_nflx_loop_s3n_hdfs(self):
    avgMichalSize = 116561140
    avgSynSize = 4020000
    csvFilenameList = [('manyfiles-nflx-gz/file_[5-9][0-9].dat.gz', 'file_50_A.dat.gz', (50 * avgMichalSize), 1800), ('manyfiles-nflx-gz/file_1[0-9][0-9].dat.gz', 'file_100_A.dat.gz', (100 * avgMichalSize), 2400), ('manyfiles-nflx-gz/file_[12][0-9][0-9].dat.gz', 'file_200_A.dat.gz', (200 * avgMichalSize), 2400), ('manyfiles-nflx-gz/file_[123][0-9][0-9].dat.gz', 'file_300_A.dat.gz', (300 * avgMichalSize), 2400), ('manyfiles-nflx-gz/file_[123][0-9][0-9].dat.gz', 'file_300_B.dat.gz', (300 * avgMichalSize), 2400), ('manyfiles-nflx-gz/file_[12][0-9][0-9].dat.gz', 'file_200_B.dat.gz', (200 * avgMichalSize), 2400), ('manyfiles-nflx-gz/file_2[0-9][0-9].dat.gz', 'file_100_B.dat.gz', (100 * avgMichalSize), 2400), ('manyfiles-nflx-gz/file_1[0-4][0-9].dat.gz', 'file_50_B.dat.gz', (50 * avgMichalSize), 1800), ('manyfiles-nflx-gz/file_1[5-9][0-9].dat.gz', 'file_50_C.dat.gz', (50 * avgMichalSize), 1800)]
    print 'Using the -.gz files from s3'
    USE_S3 = False
    noPoll = False
    benchmarkLogging = ['cpu', 'disk']
    bucket = 'home-0xdiag-datasets'
    if USE_S3:
        URI = 's3://home-0xdiag-datasets'
        protocol = 's3'
    else:
        URI = 's3n://home-0xdiag-datasets'
        protocol = 's3n/hdfs'
    trialMax = 1
    pollTimeoutSecs = 180
    retryDelaySecs = 10
    for (i, (csvFilepattern, csvFilename, totalBytes, timeoutSecs)) in enumerate(csvFilenameList):
        for tryHeap in [60]:
            print '\n', tryHeap, 'GB heap, 1 jvm per host, import', protocol, 'then parse'
            h2o_hosts.build_cloud_with_hosts(node_count=1, java_heap_GB=tryHeap, enable_benchmark_log=True, timeoutSecs=120, retryDelaySecs=10, hdfs_name_node='10.78.14.235:9000', hdfs_version='0.20.2')
            h2o.nodes[0].sandbox_ignore_errors = True
            for trial in range(trialMax):
                if USE_S3:
                    importResult = h2o.nodes[0].import_s3(bucket)
                else:
                    importResult = h2o.nodes[0].import_hdfs(URI)
                s3nFullList = importResult['succeeded']
                for k in s3nFullList:
                    key = k['key']
                    if (csvFilepattern in key):
                        print "example file we'll use:", key
                        break
                    else:
                        pass
                self.assertGreater(len(s3nFullList), 8, "Didn't see more than 8 files in s3n?")
                s3nKey = ((URI + '/') + csvFilepattern)
                key2 = (((csvFilename + '_') + str(trial)) + '.hex')
                print 'Loading', protocol, 'key:', s3nKey, 'to', key2
                start = time.time()
                parseKey = h2o.nodes[0].parse(s3nKey, key2, timeoutSecs=timeoutSecs, retryDelaySecs=retryDelaySecs, pollTimeoutSecs=pollTimeoutSecs, noPoll=noPoll, benchmarkLogging=benchmarkLogging)
                if noPoll:
                    if ((i + 1) < len(csvFilenameList)):
                        time.sleep(1)
                        h2o.check_sandbox_for_errors()
                        (csvFilepattern, csvFilename, totalBytes2, timeoutSecs) = csvFilenameList[(i + 1)]
                        s3nKey = ((URI + '/') + csvFilepattern)
                        key2 = (((csvFilename + '_') + str(trial)) + '.hex')
                        print 'Loading', protocol, 'key:', s3nKey, 'to', key2
                        parse2Key = h2o.nodes[0].parse(s3nKey, key2, timeoutSecs=timeoutSecs, retryDelaySecs=retryDelaySecs, pollTimeoutSecs=pollTimeoutSecs, noPoll=noPoll, benchmarkLogging=benchmarkLogging)
                    if ((i + 2) < len(csvFilenameList)):
                        time.sleep(1)
                        h2o.check_sandbox_for_errors()
                        (csvFilepattern, csvFilename, totalBytes3, timeoutSecs) = csvFilenameList[(i + 2)]
                        s3nKey = ((URI + '/') + csvFilepattern)
                        key2 = (((csvFilename + '_') + str(trial)) + '.hex')
                        print 'Loading', protocol, 'key:', s3nKey, 'to', key2
                        parse3Key = h2o.nodes[0].parse(s3nKey, key2, timeoutSecs=timeoutSecs, retryDelaySecs=retryDelaySecs, pollTimeoutSecs=pollTimeoutSecs, noPoll=noPoll, benchmarkLogging=benchmarkLogging)
                elapsed = (time.time() - start)
                print s3nKey, 'parse time:', parseKey['response']['time']
                print 'parse result:', parseKey['destination_key']
                print 'Parse #', trial, 'completed in', ('%6.2f' % elapsed), 'seconds.', ('%d pct. of timeout' % ((elapsed * 100) / timeoutSecs))
                if noPoll:
                    time.sleep(2)
                    h2o_jobs.pollWaitJobs(pattern=csvFilename, timeoutSecs=timeoutSecs, benchmarkLogging=benchmarkLogging)
                    totalBytes += (totalBytes2 + totalBytes3)
                    elapsed = (time.time() - start)
                    h2o.check_sandbox_for_errors()
                if (totalBytes is not None):
                    fileMBS = ((totalBytes / 1000000.0) / elapsed)
                    l = '{:d} jvms, {:d}GB heap, {:s} {:s} {:6.2f} MB/sec for {:6.2f} secs'.format(len(h2o.nodes), tryHeap, csvFilepattern, csvFilename, fileMBS, elapsed)
                    print l
                    h2o.cloudPerfH2O.message(l)
                if (not noPoll):
                    h2o_cmd.check_enums_from_inspect(parseKey)
                print 'Deleting key in H2O so we get it from S3 (if ec2) or nfs again.', 'Otherwise it would just parse the cached key.'
                storeView = h2o.nodes[0].store_view()
                h2o_cmd.delete_csv_key(csvFilename, s3nFullList)
            h2o.tear_down_cloud()
            print 'Waiting 30 secs before building cloud again (sticky ports?)'
            time.sleep(30)
