{
  epoch_counter=(float)model_info().get_processed_total() / data_info._adaptedFrame.numRows();
  boolean keep_running=(epoch_counter < model_info().parameters.epochs);
  final long now=System.currentTimeMillis();
  final long sinceLastScore=now - _timeLastScoreStart;
  final long sinceLastPrint=now - _timeLastPrint;
  if ((sinceLastPrint > 5000)) {
    final long samples=model_info().get_processed_total();
    Log.info("Training time: " + PrettyPrint.msecs(now - timeStart,true) + " processed "+ samples+ " samples"+ " ("+ String.format("%.3f",epoch_counter)+ " epochs)."+ " Speed: "+ String.format("%.3f",(double)samples / ((now - timeStart) / 1000.))+ " samples/sec.");
    _timeLastPrint=now;
  }
  if (!keep_running || (now - timeStart < 30000) || (sinceLastScore > model_info().parameters.score_interval * 1000)) {
    if (model_info.parameters.diagnostics)     computeDiagnostics();
    _timeLastScoreStart=now;
    classificationError(ftrain,"Classification error on training data:",true);
    if (ftest != null)     classificationError(ftest,"Classification error on validation data:",true);
  }
  if (model_info().unstable()) {
    Log.err("Canceling job since the model is unstable (exponential growth observed).");
    Log.err("Try using L1/L2/max_w2 regularization, a different activation function, or more synchronization in multi-node operation.");
    keep_running=false;
  }
  update(dest_key);
  return keep_running;
}
