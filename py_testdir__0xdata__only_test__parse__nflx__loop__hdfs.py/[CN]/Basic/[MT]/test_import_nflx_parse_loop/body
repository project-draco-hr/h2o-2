def test_import_nflx_parse_loop(self):
    print 'Using the -.gz files from hdfs'
    csvFilename = 'file_10.dat.gz'
    csvFilepattern = 'file_1[0-9].dat.gz'
    trialMax = 2
    for tryHeap in [24]:
        print '\n', tryHeap, 'GB heap, 1 jvm per host, import 192.168.1.176 hdfs, then parse'
        localhost = h2o.decide_if_localhost()
        if localhost:
            h2o.build_cloud(node_count=1, java_heap_GB=tryHeap, use_hdfs=True, hdfs_name_node='192.168.1.176', hdfs_version='cdh3')
        else:
            h2o_hosts.build_cloud_with_hosts(node_count=1, java_heap_GB=tryHeap, use_hdfs=True, hdfs_name_node='192.168.1.176', hdfs_version='cdh3')
        timeoutSecs = 500
        importFolderPath = 'datasets/manyfiles-nflx-gz'
        for trial in range(trialMax):
            hex_key = (((csvFilename + '_') + str(trial)) + '.hex')
            csvFilePattern = 'file_1.dat.gz'
            time.sleep(5)
            csvPathname = ((importFolderPath + '/') + csvFilePattern)
            start = time.time()
            parseResult = h2i.import_parse(path=csvPathname, schema='hdfs', hex_key=key2, timeoutSecs=timeoutSecs, retryDelaySecs=10, pollTimeoutSecs=60)
            elapsed = (time.time() - start)
            print 'parse result:', parseResult['destination_key']
            print 'Parse #', trial, 'completed in', ('%6.2f' % elapsed), 'seconds.', ('%d pct. of timeout' % ((elapsed * 100) / timeoutSecs))
            h2o_cmd.runStoreView()
        h2o.tear_down_cloud()
        time.sleep(5)
