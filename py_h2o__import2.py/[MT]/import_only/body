def import_only(node=None, schema='local', bucket=None, path=None, timeoutSecs=30, retryDelaySecs=0.5, initialDelaySecs=0.5, pollTimeoutSecs=180, noise=None, benchmarkLogging=None, noPoll=False, doSummary=True, src_key='python_src_key', **kwargs):
    if (not node):
        node = h2o.nodes[0]
    if (path is None):
        raise Exception('import_only: path parameter needs to be specified')
    if ('/' in path):
        (head, pattern) = os.path.split(path)
    else:
        (head, pattern) = ('', path)
    h2o.verboseprint('head:', head)
    h2o.verboseprint('pattern:', pattern)
    if re.search('[\\*<>{}[\\]~`]', head):
        raise Exception(("h2o folder path %s can't be regex. path= was %s" % (head, path)))
    if (schema == 'put'):
        if re.search('[/\\*<>{}[\\]~`]', pattern):
            raise Exception(("h2o putfile basename %s can't be regex. path= was %s" % (pattern, path)))
        if (not path):
            raise Exception("path= didn't say what file to put")
        (folderPath, filename) = find_folder_and_filename(bucket, path, schema)
        filePath = os.path.join(folderPath, filename)
        h2o.verboseprint('put filename:', filename, 'folderPath:', folderPath, 'filePath:', filePath)
        h2p.green_print('\nimport_only:', h2o.python_test_name, ('uses put:/%s' % filePath))
        h2p.green_print(('Local path to file that will be uploaded: %s' % filePath))
        h2p.blue_print('That path resolves as:', os.path.realpath(filePath))
        if h2o.abort_after_import:
            raise Exception("Aborting due to abort_after_import (-aai) argument's effect in import_only()")
        key = node.put_file(filePath, key=src_key, timeoutSecs=timeoutSecs)
        return (None, key)
    if ((schema == 'local') and (not (node.redirect_import_folder_to_s3_path or node.redirect_import_folder_to_s3n_path))):
        (folderPath, pattern) = find_folder_and_filename(bucket, path, schema)
        filePath = os.path.join(folderPath, pattern)
        h2p.green_print('\nimport_only:', h2o.python_test_name, ('uses local:/%s' % filePath))
        h2p.green_print(('Path h2o will be told to use: %s' % filePath))
        h2p.blue_print('If local jvms, path resolves locally as:', os.path.realpath(filePath))
        if h2o.abort_after_import:
            raise Exception("Aborting due to abort_after_import (-aai) argument's effect in import_only()")
        folderURI = ('nfs:/' + folderPath)
        importResult = node.import_files(folderPath, timeoutSecs=timeoutSecs)
    else:
        if ((bucket is not None) and re.match('/', head)):
            h2o.verboseprint('You said bucket:', bucket, "so stripping incorrect leading '/' from", head)
            head = head.lstrip('/')
        if (bucket and (head != '')):
            folderOffset = ((bucket + '/') + head)
            print 1
        elif bucket:
            folderOffset = bucket
            print 2
        else:
            folderOffset = head
        print '\nimport_only:', h2o.python_test_name, schema, 'uses', ((((schema + '://') + folderOffset) + '/') + pattern)
        if h2o.abort_after_import:
            raise Exception("Aborting due to abort_after_import (-aai) argument's effect in import_only()")
        n = h2o.nodes[0]
        if ((schema == 's3') or node.redirect_import_folder_to_s3_path):
            folderURI = ('s3://' + folderOffset)
            if (not n.aws_credentials):
                print ('aws_credentials: %s' % n.aws_credentials)
                print 'ERROR: Something was missing for s3 on the java -jar cmd line when the cloud was built'
            importResult = node.import_s3(bucket, timeoutSecs=timeoutSecs)
        elif ((schema == 's3n') or node.redirect_import_folder_to_s3n_path):
            if (not (n.use_hdfs and ((n.hdfs_version and n.hdfs_name_node) or n.hdfs_config))):
                print ('use_hdfs: %s hdfs_version: %s hdfs_name_node: %s hdfs_config: %s' % (n.use_hdfs, n.hdfs_version, n.hdfs_name_node, n.hdfs_config))
                print 'ERROR: Something was missing for s3n on the java -jar cmd line when the cloud was built'
            folderURI = ('s3n://' + folderOffset)
            importResult = node.import_hdfs(folderURI, timeoutSecs=timeoutSecs)
        elif (schema == 'maprfs'):
            if (not n.use_maprfs):
                print ('use_maprfs: %s' % n.use_maprfs)
                print 'ERROR: Something was missing for maprfs on the java -jar cmd line when the cloud was built'
            folderURI = ('maprfs:///' + folderOffset)
            importResult = node.import_hdfs(folderURI, timeoutSecs=timeoutSecs)
        elif (schema == 'hdfs'):
            if (not (n.use_hdfs and ((n.hdfs_version and n.hdfs_name_node) or n.hdfs_config))):
                print ('use_hdfs: %s hdfs_version: %s hdfs_name_node: %s hdfs_config: %s' % (n.use_hdfs, n.hdfs_version, n.hdfs_name_node, n.hdfs_config))
                print 'ERROR: Something was missing for hdfs on the java -jar cmd line when the cloud was built'
            if bucket:
                bucketAndOffset = ((bucket + '/') + folderOffset)
            else:
                bucketAndOffset = folderOffset
            if n.hdfs_name_node:
                folderURI = ((('hdfs://' + n.hdfs_name_node) + '/') + folderOffset)
            else:
                folderURI = ('hdfs://' + folderOffset)
            h2o.verboseprint(h2o.nodes[0].hdfs_name_node)
            h2o.verboseprint('folderOffset:', folderOffset)
            importResult = node.import_hdfs(folderURI, timeoutSecs=timeoutSecs)
        else:
            raise Exception(('schema not understood: %s' % schema))
    importPattern = ((folderURI + '/') + pattern)
    return (importResult, importPattern)
