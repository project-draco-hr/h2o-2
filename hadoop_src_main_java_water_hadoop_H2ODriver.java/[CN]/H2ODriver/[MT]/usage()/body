{
  String prog=H2ODriver.class.getSimpleName();
  System.err.printf("\n" + "Usage: " + prog + "\n"+ "          -files <.../flatfile.txt>\n"+ "          -libjars <.../h2o.jar>\n"+ "          [other generic Hadoop ToolRunner options]\n"+ "          [-h | -help]\n"+ "          [-jobname <name of job in jobtracker (defaults to: 'H2O_nnnnn')>]\n"+ "              (Note nnnnn is chosen randomly to produce a unique name)\n"+ "          -mapperXmx <per mapper Java Xmx heap size>\n"+ "          -n | -nodes <number of h2o nodes (i.e. mappers) to create>\n"+ "          -o | -output <hdfs output dir>\n"+ "\n"+ "Notes:\n"+ "          o  Each H2O node runs as a mapper.\n"+ "          o  All mappers must come up simultaneously before the job proceeds.\n"+ "          o  Only one mapper may be run per host (if more land on one host,\n"+ "             the subsequent mappers will exit and get rescheduled by hadoop).\n"+ "          o  Mapper output (part-n-xxxxx) is log output from that mapper.\n"+ "          o  -mapperXmx is set to both Xms and Xmx of the mapper to reserve\n"+ "             memory up front.\n"+ "          o  There are no combiners or reducers.\n"+ "          o  -files flatfile.txt is required and must be named flatfile.txt.\n"+ "          o  -libjars with an h2o.jar is required.\n"+ "          o  -mapperXmx, -n and -o are required.\n"+ "          o  Each H2O cluster should have a unique jobname.\n"+ "\n"+ "Examples:\n"+ "          "+ prog+ " -jt <yourjobtracker>:<yourport> -files flatfile.txt -libjars h2o.jar -mapperXmx 1g -n 1 -o hdfsOutputDir\n"+ "\n"+ "flatfile.txt:\n"+ "          The flat file must contain the list of possible IP addresses an H2O\n"+ "          node (i.e. mapper) may be scheduled on.  One IP address per line.\n"+ "\n");
  System.exit(1);
}
