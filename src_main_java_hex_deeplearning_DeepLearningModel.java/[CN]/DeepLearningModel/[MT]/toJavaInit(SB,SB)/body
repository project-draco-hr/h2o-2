{
  sb=super.toJavaInit(sb,fileContextSB);
  JCodeGen.toStaticVar(sb,"NUMS",new double[model_info().data_info()._nums],"Workspace for storing numerical input variables.");
  JCodeGen.toStaticVar(sb,"CATS",new int[model_info().data_info()._cats],"Workspace for storing categorical input variables.");
  JCodeGen.toStaticVar(sb,"CATOFFSETS",model_info().data_info()._catOffsets,"Workspace for categorical offsets.");
  JCodeGen.toStaticVar(sb,"NORMMUL",model_info().data_info()._normMul,"Standardization/Normalization scaling factor for numerical variables.");
  JCodeGen.toStaticVar(sb,"NORMSUB",model_info().data_info()._normSub,"Standardization/Normalization offset for numerical variables.");
  JCodeGen.toStaticVar(sb,"NORMRESPMUL",model_info().data_info()._normRespMul,"Standardization/Normalization scaling factor for response.");
  JCodeGen.toStaticVar(sb,"NORMRESPSUB",model_info().data_info()._normRespSub,"Standardization/Normalization offset for response.");
  Neurons[] neurons=DeepLearningTask.makeNeuronsForTesting(model_info());
  int[] layers=new int[neurons.length];
  for (int i=0; i < neurons.length; ++i)   layers[i]=neurons[i].units;
  JCodeGen.toStaticVar(sb,"NEURONS",layers,"Number of neurons for each layer.");
  sb.i().p("// Storage for neuron activation values.").nl();
  sb.i().p("public static final float[][] ACTIVATION = new float[][] {").nl();
  for (int i=0; i < neurons.length; i++) {
    String colInfoClazz="Activation_" + i;
    sb.i(1).p("/* ").p(neurons[i].getClass().getSimpleName()).p(" */ ");
    sb.p(colInfoClazz).p(".VALUES");
    if (i != neurons.length - 1)     sb.p(',');
    sb.nl();
    fileContextSB.i().p("// Neuron activation values for ").p(neurons[i].getClass().getSimpleName()).p(" layer").nl();
    JCodeGen.toClassWithArray(fileContextSB,null,colInfoClazz,new float[layers[i]]);
  }
  sb.i().p("};").nl();
  sb.i().p("// Neuron bias values.").nl();
  sb.i().p("public static final float[][] BIAS = new float[][] {").nl();
  for (int i=0; i < neurons.length; i++) {
    String colInfoClazz="Bias_" + i;
    sb.i(1).p("/* ").p(neurons[i].getClass().getSimpleName()).p(" */ ");
    sb.p(colInfoClazz).p(".VALUES");
    if (i != neurons.length - 1)     sb.p(',');
    sb.nl();
    fileContextSB.i().p("// Neuron bias values for ").p(neurons[i].getClass().getSimpleName()).p(" layer").nl();
    float[] bias=i == 0 ? null : new float[model_info().get_biases(i - 1).size()];
    if (i > 0) {
      for (int j=0; j < bias.length; ++j)       bias[j]=model_info().get_biases(i - 1).get(j);
    }
    JCodeGen.toClassWithArray(fileContextSB,null,colInfoClazz,bias);
  }
  sb.i().p("};").nl();
  sb.i().p("// Connecting weights between neurons.").nl();
  sb.i().p("public static final float[][] WEIGHT = new float[][] {").nl();
  for (int i=0; i < neurons.length; i++) {
    String colInfoClazz="Weight_" + i;
    sb.i(1).p("/* ").p(neurons[i].getClass().getSimpleName()).p(" */ ");
    sb.p(colInfoClazz).p(".VALUES");
    if (i != neurons.length - 1)     sb.p(',');
    sb.nl();
    if (i > 0) {
      fileContextSB.i().p("// Neuron weights connecting ").p(neurons[i - 1].getClass().getSimpleName()).p(" and ").p(neurons[i].getClass().getSimpleName()).p(" layer").nl();
    }
    float[] weights=i == 0 ? null : new float[model_info().get_weights(i - 1).rows() * model_info().get_weights(i - 1).cols()];
    if (i > 0) {
      final int rows=model_info().get_weights(i - 1).rows();
      final int cols=model_info().get_weights(i - 1).cols();
      for (int j=0; j < rows; ++j)       for (int k=0; k < cols; ++k)       weights[j * cols + k]=model_info().get_weights(i - 1).get(j,k);
    }
    JCodeGen.toClassWithArray(fileContextSB,null,colInfoClazz,weights);
  }
  sb.i().p("};").nl();
  return sb;
}
